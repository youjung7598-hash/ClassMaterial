**랭체인에 관한 용어**

- **임베딩(Embedding)**  
    문장을 숫자 벡터로 변환하는 기술. 이렇게 하면 의미적으로 비슷한 문장끼리 벡터 공간에서 가깝게 위치합니다. → 검색이나 분류에 활용.
    
- **청크(Chunk)**  
    긴 문서를 모델이 처리할 수 있도록 잘게 나눈 텍스트 조각.
    - 예: 100페이지짜리 PDF를 그대로 넣는 대신, 512토큰 단위로 나누어 저장.
        
- **청크 사이즈(Chunk Size)**  
    한 조각(Chunk)의 길이. 보통 토큰 수로 정합니다.
    - 작은 사이즈: 검색 정확도↑, 문맥 연결성↓
    - 큰 사이즈: 문맥 연결성↑, 불필요한 정보↑
        
- **청크 오버랩(Chunk Overlap)**  
    청크를 나눌 때 겹치게 하는 부분.
    - 예: chunk_size=512, overlap=128이면,  
        1번째 조각은 0~512, 2번째 조각은 384~896 토큰.
    - 이렇게 하면 경계에 걸친 문장이 잘리지 않고 문맥 유지 가능.
        
- **RAG (Retrieval-Augmented Generation)**  
    외부 문서에서 관련 내용을 검색(Retrieval)한 뒤, LLM에게 답변 생성(Generation)을 맡기는 구조. → 사내 문서 Q&A, 개인 노트 검색 등에 사용.

청크 사이즈라는 건 PDF 파일을 올려놓는 것과는 별개로, 불러온 텍스트를 토큰 단위로 잘라서 처리할 때의 조각 크기를 의미합니다.
그렇다고 꼭 PDF에 국한된것은 아닙니다. 

왜 청크(Chunk)와 오버랩(Overlap)이 필요한가?
- LLM은 긴 텍스트 전체를 한 번에 다루기 어려움 → 토큰 제한 존재.
- 따라서 문서를 작은 단위(청크)로 나눠 벡터DB에 저장해야 검색이 가능해집니다.
- 단순 PDF뿐만 아니라 모든 긴 텍스트 데이터(크롤링, Word, 이메일, 로그 등)에 동일하게 적용됩니다.

---
경우별 정리

`1)` PDF 기반 데이터
- PDF → 텍스트 추출
- 보통 긴 문서이므로 → 청크 사이즈 / 오버랩 설정 필요
- 예: 규정집, 논문, 보고서, 매뉴얼 등
    
`2)` 크롤링 기반 데이터
- 웹에서 긁어온 HTML, 뉴스 기사, 블로그 글 등 → 텍스트 추출
- 문서 단위로 보면 PDF와 똑같이 긴 텍스트가 될 수 있음
- → 역시 청크 분할 + 오버랩을 적용해야 검색 정확도 향상
- 다만 뉴스 기사처럼 짧은 텍스트는 청크 분할 없이도 OK

---
흐름을 단계별로 보면
1. PDF 파일 업로드
    - 보통은 PyPDF 같은 라이브러리로 PDF → 텍스트 추출.
        
2. 텍스트를 그대로 쓰면 문제 발생
    - LLM이나 벡터DB에 바로 넣기엔 텍스트가 너무 길어요.
    - 예: 50페이지 매뉴얼을 한 덩어리로 넣으면 검색도 안 되고 토큰 초과 오류도 납니다.
        
3. 텍스트 → 청크 분할
    - 여기서 `청크 사이즈`를 정하는 거예요.
    - 예: `chunk_size=512`라면 “512토큰 단위”로 잘라서 저장.
    - 이렇게 하면 긴 문서도 검색/질의응답에 적합해집니다.
        
4. 청크 오버랩 적용
    - 겹치게 잘라두면 문맥이 끊기지 않음.
    - 예: 첫 번째 조각(0~512), 두 번째 조각(384~896) → 128토큰이 겹침.

코딩을 예시로 보면
```python
from llama_index.node_parser import SentenceSplitter

# 문서를 512토큰 단위로 자르고, 50토큰 겹치게 설정
text_splitter = SentenceSplitter(
    chunk_size=512,
    chunk_overlap=50
)

nodes = text_splitter.get_nodes_from_documents(documents)
```
즉, PDF를 폴더에 올린다고 자동으로 청크가 나눠지는 게 아니라,  
PDF → 텍스트 추출 → 코드로 chunk_size를 설정해서 나눠야 합니다.

---
청크 사이즈 실험을 한 자료를 토대로 정리한 내용이니 참고해 주세요.

청크 사이즈 실험
- 작을 때 (256 토큰): 검색 정확도는 좋지만 문맥 단절 → 65% 정확도.
- 중간 (512 토큰): 문맥과 효율의 균형 → 82% 정확도 (최적).
- 큰 사이즈 (1024 토큰): 문맥은 풍부하나 불필요 정보 ↑ → 78% 정확도.
- 결론: 한국어 문서에는 512 토큰이 가장 효율적.

청크 오버랩 실험
- 오버랩 없음: 65% 정확도.
- 10% (50토큰): 75%.
- 25% (128토큰): 86%.
- 50% (256토큰): 88% (좋지만 비용 상승).
- 결론: 25% 오버랩(128 토큰)이 성능·비용 균형이 가장 좋음.

###### 최적 조합 성능 비교
| 조합                      | 정확도 | 속도  | 비용    |
| ----------------------- | --- | --- | ----- |
| HF 임베딩 + 256 + 0%       | 45% | 빠름  | 매우 저렴 |
| HF 임베딩 + 512 + 25%      | 62% | 중간  | 저렴    |
| OpenAI 임베딩 + 256 + 0%   | 72% | 빠름  | 중간    |
| ==OpenAI 임베딩 + 512 + 25%==  | ==88%== | ==중간==  | ==중간==    |
| OpenAI 임베딩 + 1024 + 50% | 85% | 느림  | 비쌈    |
Best Practice: OpenAI 임베딩 + 512 사이즈 + 25% 오버랩

---

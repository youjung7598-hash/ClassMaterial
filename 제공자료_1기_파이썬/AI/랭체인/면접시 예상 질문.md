
**LangChain + RAG(검색 증강 생성)** 기술관련

질문1] **인공지능 프로젝트에서 가장 중요한 결정은 무엇이었나요?**
모범답변] 임베딩 모델 선택 + 청크 사이즈/오버랩 실험 → 성능 향상 경험을 사례로 제시
어필 포인트] 데이터 기반 의사결정 능력, 실험적 접근 방식

---
포트폴리오 차별화 포인트
- A/B 테스트 결과: 청크 사이즈·오버랩·임베딩 모델 조합별 성능 비교 그래프 제시.
- 도메인 특화 최적화: 한국어 문서, 법률/금융/의료 등 도메인별 최적값 제안.
- 비용 효율성 분석: 성능 향상 vs API 비용 → ROI 관점 설명.
- 오픈소스 기여: 대부분 영어 기준(256토큰)에 맞춰진 RAG 프로젝트에 한국어 최적화 코드 PR → GitHub 기여도 쌓기.

용어:
- 도메인 = 내가 다루는 데이터가 속한 분야(법률, 금융, 의료 등)
- 도메인 특화 최적화 = 그 분야 데이터 특성을 고려해 모델/청크 전략/검색 방식을 조정하는 것
- ROI 관점 = 돈과 자원을 얼마나 효율적으로 썼는지 보는 시각
	- GPT-4 대신 GPT-4o mini 사용 → 성능은 95% 유지, 비용은 1/5 → ROI가 높다
	- 청크 최적화 실험 → 정확도 +20%, API 호출 비용 동일 → ROI 상승
- 오픈소스 기여 = GitHub 같은 공개 저장소에 내가 직접 수정·개선한 코드를 올려서 프로젝트에 기여한다는 뜻
- GitHub 기여도 쌓기 = 이렇게 하면 단순히 내 프로젝트에서만 썼다가 아니라, 공식 오픈소스에 반영될 수도 있고, 내 GitHub 프로필에 기여 이력이 남음.
	→ 취업 포트폴리오에서 실제 오픈소스 생태계에 기여한 경험으로 강하게 어필 가능.
---
면접 준비 질문 예시
- RAG 시스템에서 가장 중요한 성능 요소는?
- 임베딩 모델 선택 기준은 무엇인가?
- 청크 사이즈/오버랩을 어떻게 최적화했는가?
- 한국어 문서 처리 시 어떤 도전을 겪었고 어떻게 해결했는가?

한국어 문서에 맞는 청크 사이즈(512)와 오버랩(25%) 최적화 실험 경험을 포트폴리오와 면접에서 강조하면, 단순 구현이 아닌 데이터 기반 성능 최적화 능력을 보여줄 수 있습니다.

---
GPT같은 생성형 AI API를 프로젝트에 붙여서 돌아가게는 했는데 내부 원리를 이해 못하는 상황이라면 반드시 이해 해야할 필수 포인트를 정검하세요.

 **아키텍처 흐름 이해**
- 프론트엔드/사용자 입력 → **백엔드(Django/FastAPI/DRF)** → **AI 모델(API or 로컬모델)** → 응답 반환 → 화면 표시.
- 내 서버가 AI 모델을 직접 실행했는지, 아니면 외부 API를 호출했는지를 명확히 설명할 수 있어야 함.
**API 호출 구조**
- 어떤 함수/클래스로 API를 불렀는지 
- 입력(프롬프트) → 출력(응답)의 흐름.
**내가 한 역할**
- 연결 부분: 모델 불러오기, API 키 관리, 요청/응답 포맷 맞추기.
- 서비스화: 단순히 모델만 쓰는 게 아니라, 결과를 사용자 인터페이스(영상 자막, 영수증 데이터 등)에 반영한 부분.
- 운영 관리: 배포 환경(Docker, 서버)에서 돌아가게 만든 설정.
**AI 모델 관련 이해**
- 출처: OpenAI API인지, GitHub 공개 모델인지, Hugging Face 모델인지.
- 모델의 입력/출력 형식: (예: 영상 → keypoints → 텍스트 / 이미지 → 인식된 문자열 JSON)
- 한계와 문제점: 정확도, 속도, 언어 지원, GPU 필요 여부.
- 비용/리소스 고려: API라면 토큰·분당 비용 / 로컬이라면 GPU·메모리 부담.
**보안 및 안정성**
- API 키 관리 (`.env`, Secrets).
- 사용자 입력 검증(영상/QR 같은 경우 악성 입력 방지).
- 에러 처리 및 fallback 전략(모델 응답이 비정상일 때 대체 방법).
**사용 사례별 특징을 예를 들어**
- 실시간 수어 인식 모델
    - 어떻게 WebRTC/영상 스트림을 받아와서 keypoint를 추출했는지.
    - 어떤 라이브러리/모델(MediaPipe, OpenCV, TFLite 등)을 사용했는지.
    - 지연(latency)을 줄이기 위해 어떤 처리를 했는지.
- 영수증 QR 인식 모델
    - 어떤 오픈소스 라이브러리를 불러왔는지(PyTorch 모델, Tesseract(테서랙트) OCR, PaddleOCR(패들 오씨알) 등).
    - 결과를 어떤 형태(JSON, 텍스트)로 변환해서 DB에 저장했는지.
    - 데이터 후처리(금액/날짜/상품명 정규화)를 어떻게 했는지.
**내가 배운 점 (면접용 정리)**
- 모델을 그냥 불러다 쓴 것이 아니라,
    - 서비스에 적합하게 연결 (API 호출 → DB 저장 → UI 표시)
    - 문제 해결 경험 (속도, 비용, 정확도 개선)
    - AI의 한계와 보완 (후처리, 룰 기반, 캐싱)  
        → 이 부분을 준비해야 면접에서 깊이 있는 답변 가능.

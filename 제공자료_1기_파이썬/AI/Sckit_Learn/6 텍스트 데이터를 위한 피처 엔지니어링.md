피처(Feature)란?
- 의미: 데이터에서 관찰할 수 있는 속성, 특성, 변수
- 쉽게 말하면: 머신러닝 모델이 학습할 때 참고하는 입력 정보
    - 키, 몸무게, 성별, 나이
    - 텍스트 데이터라면: 단어 수, 문장 길이 등
    - 이미지라면: 색상 값, 픽셀 위치 등

예: "키와 몸무게로 비만 여부 예측하기"  
	→ 피처: 키, 몸무게  
	→ 정답(타깃): 비만 여부 (Yes/No)

---
피처 엔지니어링(Feature Engineering) 이란?
	데이터를 모델이 잘 이해하고 예측할 수 있도록 피처를 가공/추가/선택/변환하는 과정입니다. 즉, 데이터를 모델이 잘 학습하고 예측할 수 있도록 '의도적으로 가공하는 모든 과정을 말합니다.

피처 엔지니어링의 범위
- 사전학습된 것 사용 (Pre-trained Features)
    - 예: 사전학습된 임베딩(Word2Vec, BERT 벡터)
    - 이미지의 경우 사전학습된 CNN 특징 벡터  
        → 이미 다른 데이터로 학습한 피처를 가져와서 쓰는 경우
        
- 수작업으로 만든 것 (Manual / Rule-based)
    - 예: 직접 만든 감정 사전, 단어 빈도, 길이, 특정 키워드 포함 여부
    - 도메인 지식을 활용해 의미 있는 피처를 정의
        
- 자동으로 추출한 것 (자동 피처 추출)
    - 예: PCA, t-SNE, 오토인코더를 이용해 기존 데이터를 차원 축소 후 새로운 피처 생성

왜 중요한가?
- 좋은 피처를 만들면 모델 성능이 크게 향상
- 심지어 간단한 모델도 좋은 결과를 낼 수 있음

머신러닝의 유명한 격언:  
	"쓰레기를 넣으면 쓰레기가 나온다." (Garbage In, Garbage Out)  
	→ 즉, 좋은 데이터를 넣는 것이 무엇보다 중요!

---
피처 엔지니어링 과정
`1.` 데이터 이해
- 각 피처(변수)의 의미와 값의 분포(어떤 값들이 많은지, 극단적인 값이 있는지 등)를 파악
- → 어떤 전처리가 필요한지 판단하는 기본 단계
	예: `성별`, `나이`, `소득` 같은 피처가 어떤 형태의 값을 갖는지 살펴봄

---
`2.` 결측치 처리 (Missing Value Handling)
- 누락된 값(비어 있는 셀)을 채우거나 제거하는 작업
    - 평균/중앙값으로 채우기
    - 혹은 해당 행/열 제거
	예: `나이`가 비어 있으면 평균 나이로 채우기

```python
df["age"].fillna(df["age"].mean())
```
---
`3.` 이상치 처리 (Outlier Handling)
- 너무 크거나 작은 비정상적인 값을 탐지하고 처리
    - 값 제한하기, 제거하기, 평균으로 바꾸기 등
	예: `몸무게`가 1000kg이면 이상치 → 제거하거나 수정

---
`4.` 범주형 데이터 처리
- 문자를 모델이 이해할 수 있게 숫자로 변환
    - 방법: Label Encoding, One-Hot Encoding
	예: `"남성", "여성"` → `0, 1` 또는 `[1, 0], [0, 1]`로 변환
---
`5.` 피처 스케일링 (Feature Scaling)
- 모든 피처 값을 비슷한 범위로 조정하여 학습 효과를 높임
    - `StandardScaler`, `MinMaxScaler` 사용
	예: `나이(20~70)`와 `소득(100만~5000만)`은 범위가 다르므로 맞춰줌
---
텍스트 데이터에서의 피처 추출은 왜 필요한가?
- 텍스트는 숫자가 아니라 문자이기 때문에  
    머신러닝 모델은 바로 이해할 수 없음
- 따라서, 텍스트를 숫자로 바꾸는 작업(벡터화)이 필요함
---
텍스트 벡터화 방법 3가지
 `1.` Bag of Words (BoW)
- 각 단어가 몇 번 등장했는지(빈도)를 기준으로 숫자 벡터로 변환
- 간단하지만, 단어의 순서나 의미는 무시됨
```python
"나는 밥을 먹었다" → [1,1,1,1]  
"밥을 먹었다 나는" → [1,1,1,1] (같은 결과)
```

`2.` TF-IDF (Term Frequency - Inverse Document Frequency)
- 단어의 중요도를 반영하는 방식
- 많이 나오는 흔한 단어는 가중치 낮게, 특정 문서에서만 자주 나오는 단어는 가중치 높게
```python
“그리고”, “하지만” → 낮은 중요도  
“할리우드”, “이상한 나라” → 높은 중요도
```
---
`3.` Word Embedding
- 단어를 의미 있는 벡터 공간의 좌표로 바꿈
- 비슷한 단어는 비슷한 위치에 놓임 → 관계 이해 가능
- 대표 모델:  
    `Word2Vec`, `GloVe`, `FastText` 등
> 예:  
> `king - man + woman ≈ queen` → 관계까지 학습 가능!

---
텍스트 데이터를 수치 벡터(피처)로 변환하고, 단어의 등장 빈도에 따라 피처를 필터링하는 전체 과정을 보여주는 예제로, 
텍스트 데이터 전처리 + 피처 엔지니어링하는 코드입니다.
쉽게 말해서, 문장을 머신러닝 모델이 이해할 수 있는 숫자 형태(행렬)로 바꾸는 과정이에요.

왜 이런 변환이 필요한가?
- 머신러닝 모델은 글자 그대로의 문장은 이해하지 못하고 숫자만 처리할 수 있습니다. 그래서 문장을 → 숫자 벡터로 변환해야 합니다.  
    이 숫자 벡터가 바로 피처(feature)입니다.
```python
# -------------------------------------------------------------
# 1. 라이브러리 불러오기
# -------------------------------------------------------------
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.utils import Bunch

# -------------------------------------------------------------
# 2. 데이터 준비
# -------------------------------------------------------------
# 예제용 간단한 4문장 리스트 생성
data = [
    "This is the first document.",
    "This document is the second document.",
    "And this is the third one.",
    "Is this the first document?",
]

# scikit-learn 데이터셋 형식과 동일하게 Bunch 객체로 래핑
newsgroups_train = Bunch(data=data)

# -------------------------------------------------------------
# 3. 벡터화 도구(피처 추출기) 초기화
# -------------------------------------------------------------
# CountVectorizer: 단어 등장 횟수를 기반으로 피처 생성 (BoW 방식)
count_vectorizer = CountVectorizer()

# TfidfVectorizer: 단어 등장 빈도 + 문서 내 중요도를 반영한 피처 생성 (TF-IDF 방식)
tfidf_vectorizer = TfidfVectorizer()

# -------------------------------------------------------------
# 4. BoW 피처 추출
# -------------------------------------------------------------
# 문서 집합을 BoW 행렬로 변환
X_count = count_vectorizer.fit_transform(newsgroups_train.data)

# -------------------------------------------------------------
# 5. TF-IDF 피처 추출
# -------------------------------------------------------------
# 문서 집합을 TF-IDF 행렬로 변환
X_tfidf = tfidf_vectorizer.fit_transform(newsgroups_train.data)

# -------------------------------------------------------------
# 6. 추출된 피처(단어 벡터)의 차원 확인
# -------------------------------------------------------------
# (문서 개수, 고유 단어 개수) 형태로 출력
print("BoW 피처 shape:", X_count.shape)
print("TF-IDF 피처 shape:", X_tfidf.shape)

# -------------------------------------------------------------
# 7. BoW에서 추출된 단어 목록 확인
# -------------------------------------------------------------
feature_names = count_vectorizer.get_feature_names_out()

# 처음 1000개의 단어 출력 (여기서는 실제 단어 개수가 적어서 모두 출력됨)
print("처음 100개의 단어:", feature_names[:1000])

# -------------------------------------------------------------
# 8. BoW 행렬 출력 (각 문서에서 단어 등장 횟수)
# -------------------------------------------------------------
print("BoW Features (필터링 전):")
print(X_count.toarray())
```

출력결과
```
BoW 피처 shape: (4, 9) 
TF-IDF 피처 shape: (4, 9) 
처음 100개의 단어: ['and' 'document' 'first' 'is' 'one' 'second' 'the' 'third' 'this'] 
BoW Features (필터링 전): 
[[0 1 1 1 0 0 1 0 1] [0 2 0 1 0 1 1 0 1] [1 0 0 1 1 0 1 1 1] [0 1 1 1 0 0 1 0 1]]
```
---
##### 전체 과정 요약

4개의 문장을 단어 단위로 쪼개고, 각 문장에서 단어가 몇 번 나왔는지를 세어서 행렬(숫자 데이터)로 만든 것입니다.

여기서 중요한 건,
- CountVectorizer → 단어의 등장 횟수만 기록
- TfidfVectorizer → 단어의 중요도 점수를 계산 (횟수 + 희귀성 반영)

`(1)` 단어 사전 만들기
`feature_names = count_vectorizer.get_feature_names_out()`

- 모든 문서(4개 문장)에 나오는 고유 단어를 모아 목록을 만듭니다.
- 이 목록이 행렬의 열 이름이 됩니다.
- 예: `['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']`
    

`(2)` CountVectorizer 실행
`X_count = count_vectorizer.fit_transform(newsgroups_train.data)`

- 각 문서(행) × 각 단어(열) 형태의 단어 빈도 행렬 생성
- 값 = 해당 문서에서 단어가 몇 번 나왔는지

| 문서<br>번호 | and | docu-<br>ment | first | is  | one | second | the | third | this |
| -------- | --- | ------------- | ----- | --- | --- | ------ | --- | ----- | ---- |
| 0        | 0   | 1             | 1     | 1   | 0   | 0      | 1   | 0     | 1    |
| 1        | 0   | 2             | 0     | 1   | 0   | 1      | 1   | 0     | 1    |
| 2        | 1   | 0             | 0     | 1   | 1   | 0      | 1   | 1     | 1    |
| 3        | 0   | 1             | 1     | 1   | 0   | 0      | 1   | 0     | 1    |

`(3)` TfidfVectorizer 실행
`X_tfidf = tfidf_vectorizer.fit_transform(newsgroups_train.data)`

- CountVectorizer와 같은 구조지만 값이 다릅니다.
- TF-IDF 공식:
`TF(단어 빈도) × IDF(역문서 빈도)`

- 자주 등장하지만 모든 문서에 흔하게 있는 단어는 점수를 낮춤  
    → 예: "the", "is" 같은 불용어(Stopwords)
- 특정 문서에만 상대적으로 많이 등장한 단어의 점수를 높임

| 문서<br>번호 | and  | docu-<br>ment | first | is   | one  | second | the  | third | this |
| -------- | ---- | ------------- | ----- | ---- | ---- | ------ | ---- | ----- | ---- |
| 0        | 0.0  | 0.38          | 0.58  | 0.38 | 0.0  | 0.0    | 0.38 | 0.0   | 0.38 |
| 1        | 0.0  | 0.53          | 0.0   | 0.38 | 0.0  | 0.53   | 0.38 | 0.0   | 0.38 |
| 2        | 0.40 | 0.29          | 0.0   | 0.29 | 0.51 | 0.0    | 0.29 | 0.51  | 0.29 |
| 3        | 0.0  | 0.38          | 0.58  | 0.38 | 0.0  | 0.0    | 0.38 | 0.0   | 0.38 |
위의 표는 TF-IDF 점수이고 범위가 보통 0~1사이로 나오며 1에 가까울수록 해당 문서에서 중요한 단어로 간주됩니다.

중요한 점
- 단어 순서대로 정렬한것입니다.  
    → 정렬 기준은 "사전 순" (알파벳 순)이지, 중요도 순은 아닙니다.
- CountVectorizer의 `get_feature_names_out()` 결과는 단어를 사전순으로 나열한 것입니다.
- TF-IDF 값이 높은 단어가 그 문서에서 중요한 단어입니다.  
---
##### 머신러닝 전체 흐름에서 피처데이터 코드의 위치
`1.` 데이터 수집
    - 원시 데이터(문장, 이미지, 센서 데이터 등)를 모음
        
`2. `데이터 전처리
    - 불필요한 기호, 공백 제거
    - 결측치 처리, 정규화 등
        
`3.` 피처 엔지니어링 ==← 지금 코드==
    - 원시 데이터를 모델이 학습할 수 있는 숫자 벡터(피처)로 변환
    - 텍스트라면 BoW, TF-IDF, 임베딩 등을 사용해 변환
    - 이미지라면 픽셀 값이나 CNN 특징 벡터로 변환
        
`4.` 모델 학습
    - 변환된 숫자 벡터(X)와 정답(y)을 가지고 학습
    - 예: `model.fit(X_train, y_train)`
        
`5.` 모델 평가
    - 테스트 데이터로 성능 확인

----
첫 번째 코드 — 기본 피처 추출
첫 번째 코드는 "문장 → 숫자 피처"로 바꾸는 기본 단계였습니다.
목적: 문장을 머신러닝이 이해할 수 있는 숫자 데이터(피처 데이터)로 변환하는 것
1. CountVectorizer (BoW)
    - 각 문장에서 단어가 몇 번 나왔는지 세어 숫자 행렬 생성
2. TfidfVectorizer (TF-IDF)
    - 단어의 중요도 점수를 계산하여 숫자 행렬 생성
3. 결과
    - 모든 단어를 그대로 사용 (불필요한 단어도 포함)
    - 예: "is", "the" 같은 흔한 단어도 그대로 포함됨

그렇다면 아래 코드는 필터링 기능을 추가한 업그레이드 코드입니다.
```python
# ==============================================================
# 기존 BoW(단어 등장 횟수) 코드에서 업그레이드된 부분:
# -모든 단어를 사용하는 대신, 전체문서에서 최소 N번 이상 등장한 단어만 사용
# -이렇게 하면 드물게 등장하는 불필요한 단어(노이즈)를 제거하고
#  모델 학습 시 더 간결하고 중요한 피처만 사용할 수 있음
# -업그레이드 핵심 단계: 4~6번 (단어 빈도 계산 → 필터링 → BoW 재생성)
# ==============================================================


# ------------------------------------------------------------
# 1. 라이브러리 불러오기
# ------------------------------------------------------------
from sklearn.feature_extraction.text import CountVectorizer

# ------------------------------------------------------------
# 2. 샘플 데이터 준비
# ------------------------------------------------------------
# 코퍼스(corpus): 여러 개의 문서(문장) 집합
corpus = [
    "This is the first document.",
    "This document is the second document.",
    "And this is the third one.",
    "Is this the first document?",
]

# ------------------------------------------------------------
# 3. CountVectorizer로 BoW(Bag-of-Words) 변환
# ------------------------------------------------------------
# CountVectorizer: 단어 등장 횟수를 기반으로 문서를 벡터로 변환
count_vectorizer = CountVectorizer()

# fit_transform(): 단어 사전 생성 + BoW 행렬 생성
X = count_vectorizer.fit_transform(corpus)

# ------------------------------------------------------------
# 4. 전체 단어 빈도 계산 (업그레이드 1단계)
# ------------------------------------------------------------
# 기존에는 BoW 행렬만 만들고 바로 사용했지만,
# 여기서는 전체 문서에서 각 단어가 몇 번 등장했는지 합계를 계산함
word_frequencies = X.sum(axis=0)

# 단어와 해당 빈도수를 딕셔너리로 변환
word_freq_dict = dict(zip(count_vectorizer.get_feature_names_out(),
                          word_frequencies.tolist()[0]))

# 전체 단어 빈도 출력 → 어떤 단어가 몇 번 등장했는지 확인
print("전체 단어 빈도수:", word_freq_dict)

# ------------------------------------------------------------
# 5. 최소 빈도 기준으로 단어 필터링 (업그레이드 2단계)
# ------------------------------------------------------------
# 최소 빈도수 설정 (여기서는 2 이상)
# → 너무 적게 등장한 단어는 중요도가 낮을 수 있으므로 제거
min_frequency = 2

# 설정한 최소 빈도 이상인 단어만 선택
filtered_words = [word for word, freq in word_freq_dict.items() if freq >= min_frequency]

# ------------------------------------------------------------
# 6. 필터링된 단어로 BoW 벡터화 재실행 (업그레이드 3단계)
# ------------------------------------------------------------
# vocabulary 파라미터: 사용할 단어 목록만 지정
# → 불필요한 단어를 완전히 제외하고 BoW 행렬을 다시 생성
filtered_count_vectorizer = CountVectorizer(vocabulary=filtered_words)

# 필터링된 BoW 행렬 생성
X_filtered = filtered_count_vectorizer.fit_transform(corpus)

# ------------------------------------------------------------
# 7. 필터링 결과 확인
# ------------------------------------------------------------
# 남은 단어 목록 출력
print("Filtered Words:", filtered_count_vectorizer.get_feature_names_out())

# 필터링된 BoW 행렬 출력 (각 문서에서 단어 등장 횟수)
print("Filtered Features:")
print(X_filtered.toarray())

```

출력결과
```
첫번재 코드

공통점: 모두 CountVectorizer(BoW)방식으로 만들어진 피처 데이터입니다.
- 행(row) = 문서(4개 문장)   
- 열(column) = 단어(피처)   
- 값(value) = 해당 문서에서 단어가 등장한 횟수

필터링 전 (첫 번째 결과)

BoW 피처 shape: (4, 9) 
TF-IDF 피처 shape: (4, 9) 
처음 100개의 단어: ['and' 'document' 'first' 'is' 'one' 'second' 'the' 'third' 'this'] 
BoW Features (필터링 전): 
[[0 1 1 1 0 0 1 0 1] 
[0 2 0 1 0 1 1 0 1] 
[1 0 0 1 1 0 1 1 1] 
[0 1 1 1 0 0 1 0 1]]

- 열 9개 → 모든 단어 포함 (희귀 단어까지 포함)  
    예: 'and', 'one', 'second', 'third' 같은 단어도 있음   
- 행렬의 각 값은 해당 문장에서 단어가 등장한 횟수  
    예: 첫 번째 문서 `[0 1 1 1 0 0 1 0 1]`
    - 'document' 1회, 'first' 1회, 'is' 1회, 'the' 1회, 'this' 1회

--------------------------------------------
필터링 후 (두 번째 결과)

전체 단어 빈도수: {'and': 1, 'document': 4, 'first': 2, 'is': 4, 'one': 1, 'second': 1, 'the': 4, 'third': 1, 'this': 4} 
Filtered Words: ['document' 'first' 'is' 'the' 'this'] 
Filtered Features: 
[[1 1 1 1 1] 
[2 0 1 1 1] 
[0 0 1 1 1] 
[1 1 1 1 1]]

- 열 5개 → 최소 등장 횟수 2회 이상인 단어만 포함  
    → 'and', 'one', 'second', 'third' 같은 드문 단어 제거 
- 희귀 단어를 제거했기 때문에, **행렬이 더 단순해짐**  
- 첫 번째 문서 `[1 1 1 1 1]`
    - 'document' 1회, 'first' 1회, 'is' 1회, 'the' 1회, 'this' 1회 
```

피처 엔지니어링과 전처리의 차이
1. 전처리 (Data Preprocessing)
- 목적: 원본 데이터를 깨끗하고 일관성 있게 만드는 것
- 예시 작업:
    - 결측치 채우기 (`fillna`)
    - 이상치 제거
    - 텍스트에서 특수문자, 공백 제거
    - 소문자 변환
    - 불용어 제거(stopwords)
- 결과: 여전히 사람이 읽는 데이터 형태일 수도 있음 (숫자화 안 된 경우도 많음)
    
---
2. 피처 엔지니어링 (Feature Engineering)
- 목적: 모델이 잘 학습할 수 있도록 데이터를 수치형 피처로 변환하고, 필요한 특성을 추가/선택하는 것
- 예시 작업:
    - BoW, TF-IDF, Word2Vec, 이미지 CNN 벡터 등 → 숫자 벡터 변환
    - 중요 변수 선택 (Feature Selection)
    - 변수 조합으로 새로운 피처 생성 (Feature Creation)
    - 스케일링/정규화
- 결과: 반드시 모델 입력으로 바로 쓸 수 있는 수치 데이터(X)
    
---
관계
- 전처리는 피처 엔지니어링 이전에 주로 수행
- 전처리를 거친 데이터를 가지고 피처 엔지니어링을 함
- 일부 작업은 두 영역에 모두 걸칠 수 있음 (예: 불용어 제거 → 전처리이자 피처 엔지니어링의 일부로도 볼 수 있음)
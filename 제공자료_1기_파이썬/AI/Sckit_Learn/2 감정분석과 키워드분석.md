데이터에서 감정을 읽어내는 법 (감정분석, Sentiment Analysis)

`1.` 어떤 데이터를 분석하나요?
- 우리가 다루는 텍스트 데이터는 보통 이런 것들입니다.
    - 소셜 미디어 댓글
    - 상품 리뷰
    - 고객 피드백

예시:
	“이 제품은 다른 제품에 비해 기능의 수는 적지만, 딱 필요한 기능들이 완벽하게 작동합니다.”
    이런 문장에는 감정이 담겨 있어요. 

예를 들면:
    - 기능이 적다 → 살짝 부정적?    
    - 필요한 기능이 완벽 → 긍정적!
        

`2.` 감정분석이란 무엇인가요?
- 감정분석이란, 문장 속 감정을 자동으로 파악해서 분류하는 거예요.
- 분류 방식은 주로 이렇게 나뉘어요:
    - 긍정
    - 부정
    - 중립

`3.` 어떻게 분석하나요?
- 자연어 처리(NLP: Natural Language Processing) 기술을 이용해요.
- 이 기술을 통해서 텍스트 안에 있는 패턴(단어, 문장 구조 등) 을 뽑아내고,
- 그걸 바탕으로 감정이 긍정인지, 부정인지 예측하는 거예요.
    
사이킷런에서는 이렇게 해요:
- 텍스트를 숫자로 바꾸는 작업 (예: `CountVectorizer`, `TfidfVectorizer`)
- 감정을 분류하는 모델 학습 (예: `LogisticRegression`, `NaiveBayes`)
- 학습된 모델로 새로운 문장의 감정 예측

---
기술 접근전 알아야 할 핵심 단어 정리

기술 접근 전 핵심 단어 살펴보기 (텍스트 분류 및 감정분석의 기초)
1. 토크나이징 (Tokenizing)  ==-> 전처리개념==
    → 문장을 단어 하나하나로 쪼개는 작업이에요.  
    예: `"나는 밥을 먹었다"` → `["나","는","밥","을","먹","었","다"]`
    `나`    명사 (주어)
	`는`	 조사 (주격조사)
	`밥`	 명사 (목적어)
	`을`	 조사 (목적격조사)
	`먹`	 동사 어간
	`었`	 과거 시제 어미
	`다`	 종결 어미
    
2. 정제 (Cleaning)  ==-> 전처리개념==
    → 텍스트 안에 있는 불필요한 기호, 숫자, 태그, 특수문자 등을 제거해서 깔끔하게 만드는 과정이에요.  
    예: `"안녕하세요!!! ^_^"` → `"안녕하세요"`
    
3. 정규화 (Normalization)  ==-> 전처리개념==
    → 뜻은 같지만 형태가 다른 단어들을 하나로 통일하는 작업이에요.  (시제통일)
    예: `"run"`, `"ran"`, `"running"` → `"run"`
    
4. 스탑워드 (Stop Words)  ==-> 전처리개념==
    → 의미 분석에 별로 도움이 되지 않는 자주 쓰이는 일반적인 단어들은 제외해요.  
    예: `"the"`, `"is"`, `"at"` 등 
    (관사,be동사,전치사)는 의미기여도가 낮고 단순 연결역할 또는 문맥 중요도가  낮을때 제외합니다.
    
5. Bag of Words (BoW)  ==-> 기계가 이해하고 분류하는 개념(벡터화 및 알고리즘)==
    → 단어들의 등장 횟수만 세서 문장을 표현하는 방식이에요.  
    문장의 문법이나 순서는 고려하지 않아요.  
    예: `"I love you"`와 `"you love I"`는 동일하게 표현됨
    
6. TF-IDF (Term Frequency-Inverse Document Frequency)  
    → 문서에서 특정 단어가 얼마나 중요한지를 수치로 계산해줘요. 
    ==-> 기계가 이해하고 분류하는 개념(벡터화 및 알고리즘)==
    - 많이 등장할수록 중요하지만
    - 여러 문서에 다 등장하는 흔한 단어는 중요도 낮게 평가됨
    - 어떤 단어가 문서 내에서 많이 등장(TF)하면 중요하다고 보되,
    - 여러 문서에 공통으로 다 등장(IDF)하면 흔한 단어니까 덜 중요하다고 판단하는 방식
        
7. 나이브 베이즈 (Naive Bayes)   
    → 단어들의 등장 확률을 이용해서 문장이 어떤 분류에 속할지 예측해요.  
    ==-> 기계가 이해하고 분류하는 개념(벡터화 및 알고리즘)==
    감정분석 같은 텍스트 분류에 효과적이에요.
    
8. 로지스틱 회귀 (Logistic Regression)  
    → 어떤 문장이 긍정인지 부정인지처럼 둘 중 하나로 분류할 때 사용해요.  
    ==-> 기계가 이해하고 분류하는 개념(벡터화 및 알고리즘)==
    확률로 예측하고, 간단하면서도 자주 쓰이는 모델이에요.
    
9. 서포트 벡터 머신 (SVM)  
    → 데이터를 구분할 최적의 선(경계선) 을 찾아서 분류해주는 강력한 알고리즘이에요.  
    ==-> 기계가 이해하고 분류하는 개념(벡터화 및 알고리즘)==
    복잡한 문제에서도 성능이 좋아요.
    
10. 정확도 (Accuracy)  
    → 전체 데이터 중에서 맞춘 정답 비율이에요.  
	==-> 기계가 이해하고 분류하는 개념(벡터화 및 알고리즘)==
    예: 100개 중 90개 맞추면 정확도는 90%
    
11. 정밀도 (Precision)  ==-> 기계가 이해하고 분류하는 개념(벡터화 및 알고리즘)==
    → "맞다고 예측한 것들 중 실제로 맞은 비율"  
    예: 모델이 긍정이라고 한 것들 중, 실제 긍정은 몇 개인가?
    
12. 재현율 (Recall)  ==-> 기계가 이해하고 분류하는 개념(벡터화 및 알고리즘)==
    → "실제 정답 중에서 얼마나 많이 맞췄는지"  
    예: 실제 긍정이 10개 있었는데, 모델이 그중 8개를 맞췄다면 재현율은 80%
    
13. F1 점수 (F1 Score)  ==-> 기계가 이해하고 분류하는 개념(벡터화 및 알고리즘)==
    → 정밀도와 재현율을 동시에 고려한 종합 점수예요.
    - 둘 중 하나가 너무 낮으면 F1도 낮아져요.
    - 불균형 데이터 분류 평가에 유용

---
텍스트에서 감정을 분석하는 기술적인 접근 방법(쉽게 말하면, 감정분석을 어떻게 하느냐!)

`1.` 데이터 전처리
- 지저분한 텍스트를 깨끗하게 정리하는 과정이에요.
- 예: 특수문자 제거, 띄어쓰기 정리, 단어 쪼개기(토크나이징), 불필요한 단어 삭제 등
- ✔️ 텍스트를 컴퓨터가 이해하기 쉽게 바꿔주는 단계예요.
    

`2.` 특성 추출
- 컴퓨터가 문장을 숫자로 이해할 수 있도록 바꾸는 과정이에요.
- 대표적인 방법:
    - Bag of Words
    - TF-IDF
- ✔️ 텍스트 → 숫자 벡터로 변환해줘야 머신러닝에 쓸 수 있어요.
    
`3.` 모델 선택 및 훈련
- 변환된 데이터를 머신러닝 모델에 넣고 학습시키는 단계예요.
- 사용되는 모델 예:
    - Naive Bayes
    - Logistic Regression
    - SVM 등
- ✔️ 모델이 학습을 통해 “이런 문장은 긍정일 확률이 높다” 같은 규칙을 배우게 돼요.
    
`4.` 성능 평가
- 모델이 얼마나 잘 맞추는지 테스트하는 단계예요.
- 평가 지표:
    - 정확도(Accuracy)
    - 정밀도(Precision)
    - 재현율(Recall)
    - F1 Score 등
- ✔️ 실제로 쓸만한 모델인지 확인하는 마지막 점검이에요.
---
정리하면 감정분석은  
(1) 텍스트 정리 → (2) 숫자로 변환 → (3) 모델에 학습 → (4) 성능 체크  
라는 4단계를 반복해서 더 좋은 모델을 만드는 과정이입니다.

---
✅ Scikit-Learn을 활용한 키워드 추출이란?
키워드 분석이란?  
→ 문장이나 문서에서 핵심적인 단어(=키워드) 를 뽑아내는 작업이에요.

왜 중요할까?
- 데이터 분석: 사람들이 많이 언급하는 주제를 파악할 수 있어요.
- SEO (검색엔진 최적화): 어떤 단어를 강조해야 검색이 잘 되는지 알 수 있어요.
- 콘텐츠 마케팅: 사람들이 관심 갖는 주제를 뽑아 콘텐츠를 만들 수 있어요.
    
Scikit-Learn의 역할은?  
→ 파이썬의 머신러닝 도구인 Scikit-Learn을 사용하면,  
단어의 중요도를 계산해서 자동으로 키워드를 추출할 수 있어요.

---
✅ 텍스트 데이터에서 키워드 추출 방법
1. TF-IDF 방식 (Term Frequency-Inverse Document Frequency)
- 텍스트 안에서 자주 등장하지만 흔하지 않은 단어를 키워드로 뽑는 수학적인 방법이에요.
- 예를 들어,
    - "맛집", "추천", "리뷰"가 자주 등장하고
    - 다른 문서에는 잘 안 나오는 경우 → 중요한 키워드!
- Scikit-Learn에서는 `TfidfVectorizer`라는 도구로 쉽게 계산할 수 있어요.
    
비유:
- "카레 맛집 추천합니다!"라는 글이 100개 중 1개에만 나온다면,  
    "카레"는 희귀하고 중요한 단어 = 키워드!
---
2. 워드 클라우드 (Word Cloud) 생성
- 키워드들을 시각화하는 방법이에요.
- 자주 나오는 단어는 크게, 덜 나오는 단어는 작게 보여줘요.
- 글의 핵심 키워드가 한눈에 보이게 해주는 시각 도구예요.
    
비유:
- 친구 대화 내용을 워드 클라우드로 보여주면  
    "여행", "맛집", "카페"가 크게 보이면 = 그 주제들이 자주 언급됐다는 뜻이에요.

---
실습하기

한국어 형태소 분석기 설치
```bash
pip install konlpy
sudo apt update
sudo apt install default-jdk
```

형태소 분석기란?
문장을 가장 작은 의미 단위인 ‘형태소’로 쪼개주는 도구예요

왜 필요한가요?
- 영어는 단어마다 띄어쓰기가 뚜렷하지만,  
    한국어는 한 단어에 여러 의미가 붙어서 복잡해요.  
    예: `"먹었습니다"` → `먹 (동사) + 었 (과거) + 습니다 (높임말)`
- 그래서 한국어 문장을 잘게 나눠서 분석하려면 형태소 분석이 꼭 필요해요.

형태소란?
- 뜻이 있는 최소 단위예요.
- 예: `"사랑합니다"` → `사랑 (명사) + 하 (동사) + ㅂ니다 (어미)`

형태소 분석기 역할:
- 문장에서 단어별 의미 단위를 구분해줘요.
- 예: `"배가 아파요"` → `배 (명사) + 가 (조사), 아프 (형용사 어간) + 아요 (어미)`

언제 사용하나요?
- 감정분석, 키워드 추출할 때 중요한 단어를 정확히 뽑기 위해 사용해요.

###### `KoNLPy`는 파이썬에서 사용할 수 있는 한국어 형태소 분석기 라이브러리
| 이름             | 특징                                                               | 사용 예시            |
| -------------- | ---------------------------------------------------------------- | ---------------- |
| Mecab (메캅)     | - 가장 빠르고 정확한 분석기<br>- 대용량 텍스트 처리에 적합<br>- 설치가 조금 복잡 (특히 Windows) | 실무/서비스에 자주 사용됨   |
| Okt            | - SNS 분석에 적합<br>- 띄어쓰기 보정, 품사 태깅 잘함<br>- 초보자도 사용 쉬움              | 감정 분석, 키워드 분석    |
| Komoran (코모란)  | - Java 기반으로 만든 분석기<br>- 어근 분석, 정확한 품사 분석에 강함                     | 문장 구조 분석         |
| Hannanum (한나눔) | - 서울대에서 개발<br>- 문장 성분 분석(주어, 목적어 등) 도 가능                         | 문장 구조 연구용        |
| Kkma (꼬마)      | - 문장 분리와 복잡한 문장 분석에 강함<br>- 느리고 무겁지만 정밀함                         | 뉴스, 논문 등 정제된 텍스트 |

---
실습코드:
문장의 감정이 긍정(1)인지 부정(0)인지 자동으로 분류하는 모델 만들기

전체코드:
```python
# 필요한 라이브러리 불러오기
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# 1. 텍스트 데이터 (리뷰 문장들)
text_data = [
    "This product is great",
    "Bad quality, not satisfied",
    "Happy with the purchase",
    "Not a good product",
    "Excellent quality and fast shipping",
    "Terrible customer service, I'm disappointed",
    "Very comfortable and fits well",
    "The material feels cheap and it broke easily",
    "I love it! Would definitely recommend",
    "Not what I expected, returning it",
    "Well worth the price",
    "Product did not match the description",
    "So happy with this purchase, five stars!",
    "One star, the product stopped working after a week",
    "Size was perfect and I like the color",
    "Too small and the color is different from the picture",
    "Amazing experience, I'm very satisfied with the product",
    "It's okay, but I expected better quality",
    "Received the wrong item, not happy",
    "Fantastic! Better than I expected"
]

# 2. 정답 레이블 (1 = 긍정, 0 = 부정) → 문장 순서와 일치해야 함
labels = [1, 0, 1, 0, 1, 0, 1, 0, 1, 0,
          1, 0, 1, 0, 1, 0, 1, 0, 0, 1]

# 3. 문장을 숫자 벡터로 변환 (단어 수 기반)
vectorizer = CountVectorizer()
X = vectorizer.fit_transform(text_data)  # 문장들을 벡터로 변환

# 4. 학습용 데이터와 테스트용 데이터 나누기 (80% vs 20%)
X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2)

# 5. 모델 생성 및 학습 (나이브 베이즈 분류기)
model = MultinomialNB()
model.fit(X_train, y_train)  # 학습

# 6. 테스트 데이터로 예측
predictions = model.predict(X_test)

# 7. 정확도 평가 출력
print("Accuracy:", accuracy_score(y_test, predictions))
```
---
필요한 도구 불러오기
```python
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
```

`from sklearn.feature_extraction.text import CountVectorizer`
- 문장을 숫자 벡터(단어 수 기준) 로 바꿔주는 도구예요.  
- 예: `"I love it"` → `{"I":1, "love":1, "it":1}`

`from sklearn.naive_bayes import MultinomialNB`
- 나이브 베이즈(Naive Bayes) 라는 간단하고 빠른 분류 알고리즘입니다.  
- 텍스트 분류(스팸메일, 감정분석 등)에 자주 사용돼요.

`from sklearn.model_selection import train_test_split`
- 데이터를 훈련용과 테스트용으로 나눠주는 도구예요.

`from sklearn.metrics import accuracy_score`
- 예측 결과가 얼마나 맞았는지 정확도(Accuracy) 를 계산해줍니다.

---
텍스트 데이터와 레이블 만들기
```python
# 샘플 데이터
text_data = [
"This product is great", 
"Bad quality, not satisfied",
"Happy with the purchase",
"Not a good product",
"Excellent quality and fast shipping",
"Terrible customer service, I'm disappointed",
"Very comfortable and fits well",
"The material feels cheap and it broke easily",
"I love it! Would definitely recommend",
"Not what I expected, returning it",
"Well worth the price",
"Product did not match the description",
"So happy with this purchase, five stars!",
"One star, the product stopped working after a week",
"Size was perfect and I like the color",
"Too small and the color is different from the picture",
"Amazing experience, I'm very satisfied with the product",
"It's okay, but I expected better quality",
"Received the wrong item, not happy",
"Fantastic! Better than I expected"
]
```
- 리뷰 문장들을 리스트로 저장했어요.
✅ 긍정 문장 (Positive)
1. ✅ "This product is great"
2. ✅ "Happy with the purchase"
3. ✅ "Excellent quality and fast shipping"
4. ✅ "Very comfortable and fits well"
5. ✅ "I love it! Would definitely recommend"
6. ✅ "Well worth the price"
7. ✅ "So happy with this purchase, five stars!"
8. ✅ "Size was perfect and I like the color"
9. ✅ "Amazing experience, I'm very satisfied with the product"
10. ✅ "Fantastic! Better than I expected"
---
❌ 부정 문장 (Negative)
1. ❌ "Bad quality, not satisfied"
2. ❌ "Not a good product"
3. ❌ "Terrible customer service, I'm disappointed"
4. ❌ "The material feels cheap and it broke easily"
5. ❌ "Not what I expected, returning it"
6. ❌ "Product did not match the description"
7. ❌ "One star, the product stopped working after a week"
8. ❌ "Too small and the color is different from the picture"
9. ❌ "It's okay, but I expected better quality"
10. ❌ "Received the wrong item, not happy"
---
이 코드는 텍스트 데이터를 머신러닝 모델이 처리할 수 있도록 벡터화하고, 훈련용/테스트용 데이터로 분리하는 핵심 과정입니다.
```python
labels = [1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1] # 20개 감정 레이블 (1=긍정, 0=부정)
# 사람이 직접 문장의 감정을 보고 판단해서 일일이 붙여준 정답데이터입니다.

vectorizer = CountVectorizer()
# 모든 문장을 살펴보고,등장하는 모든 단어를 한 번씩만 모아서 단어 목록(사전)을 만들어요.
# 그런 다음,각 문장에 이 단어들이 있는지(1), 없는지(0) 또는 몇 번 나왔는지(횟수)를 세어서 숫자 리스트(벡터)로 바꿔요.


X = vectorizer.fit_transform(text_data)

X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2)
```

감정 레이블(label) 만들기
```python
labels = [1, 0, 1, 0, ..., 1]  # 총 20개
```
- 각 문장이 긍정인지(1), 부정인지(0) 표시한 정답 리스트입니다.
- 사람이 직접 문장을 보고 감정을 판단해 만든 데이터예요.
예시:
```python
"This product is great" → 1 (긍정) 
"Bad quality, not satisfied" → 0 (부정)
```

CountVectorizer 객체 생성
```python
vectorizer = CountVectorizer()
```
- 텍스트 문장을 숫자 벡터로 변환하기 위한 도구입니다.
- 문장에서 모든 단어를 한 번씩만 모아서 단어 사전(목록) 을 만들고,  각 문장을 해당 단어가 있으면 1, 없으면 0 또는 단어 수로 표현해요.

문장을 숫자 벡터로 변환
```python
X = vectorizer.fit_transform(text_data)
```
- `text_data` 안의 문장들을 `CountVectorizer`로 숫자 벡터로 바꿉니다.
- 결과는 희소 행렬(Sparse Matrix) 이며, 이게 모델의 입력값입니다.

문장 → 벡터로 변환:
```python
"This is good" → [0, 1, 1, 1] 
"This is bad"  → [1, 0, 1, 1]

단어 사전: ["bad", "good", "is", "this"]
```

훈련용 / 테스트용 데이터 분리
```python
X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2)
```
- 데이터를 무작위로 섞어 80%는 학습용, 20%는 모델 성능 평가용으로 나눕니다.
- `test_size=0.2` → 20%는 모델 평가용으로 남긴다는 뜻으로,  
- 총 20개 중 약 4개는 테스트용, 16개는 학습용이 됩니다.

---
```python
# 모델 훈련
model = MultinomialNB() # 1. 모델 준비
model.fit(X_train, y_train) # 2. 모델 학습 (훈련)
```

`MultinomialNB()`란?
	나이브 베이즈 분류기(Naive Bayes Classifier) 중 하나입니다.
특징
- 텍스트 분류에 자주 사용되는 머신러닝 알고리즘이에요.
- `Multinomial`이라는 이름은 단어 빈도(출현 횟수) 를 기반으로 작동하기 때문입니다.
- 빠르고, 구현이 쉬우며, 작은 데이터셋에서도 꽤 정확합니다.

모델학습
```python
model.fit(X_train, y_train) 
#각 단어에 대한 가중치(weight)를 모델이 스스로 계산
```
- `X_train`: 숫자로 변환된 문장들 (입력)
- `y_train`: 각 문장에 해당하는 감정 (출력: 1 또는 0)
- fit함수 안에는 가중치가 자동으로 학습됩니다.

보충설명:
가중치(Weight)란?
	각 단어가 얼마나 중요한지를 나타내는 숫자 값이에요.  
	다시 말해, 어떤 단어가 감정(긍정/부정)에 얼마나 영향을 주는지를 나타내는 수치예요.

가중치는 왜 필요할까요?
- 모든 단어가 같은 의미를 가지는 건 아니에요.
    - `"great"` 같은 단어는 긍정적인 의미를 강하게 나타내고,
    - `"terrible"`은 부정적인 느낌을 줘요.
    - 반면에 `"the"`, `"is"` 같은 단어는 감정과 관련이 없어서 영향이 거의 없어요.

딥러닝이든 머신러닝이든 모델 학습의 핵심은 "가중치(weight)"를 잘 계산하는 것입니다. 
	- 가중치가 잘 계산되면 = 모델이 잘 학습된 것
	- 딥러닝 학습이란 → 가중치를 자동으로 조정하는 과정입니다.

###### 단어 사전이 이렇게 있다고 해볼게요:
| 단어       | 문장에 포함 여부 (0/1) | 가중치  |
| -------- | --------------- | ---- |
| great    | 1               | +2.5 |
| product  | 1               | +0.5 |
| the      | 1               | 0.0  |
| terrible | 0               | -3.0 |
계산 방법:
- 문장을 벡터로 표현하면: `[1, 1, 1, 0]`  
- 각 단어의 벡터 값 × 가중치 값을 곱해서 다 더합니다:
```python
= (1 × 2.5)  +  (1 × 0.5)  +  (1 × 0.0)  +  (0 × -3.0)
= 2.5 + 0.5 + 0.0 + 0.0
= 3.0
```
점수(3.0)가 양수(+)면 긍정, 음수(-)면 부정이라고 판단합니다.

가중치 숫자의 기준은?
- 모델이 학습(훈련) 하면서 자동으로 결정해요.
- 자주 나오는 단어라고 해서 무조건 높지는 않아요.
- 긍정 문장에서 많이 나왔는지, 부정 문장에서 많이 나왔는지를 보고 결정해요.

|단어|긍정에 자주 등장|부정에 자주 등장|결과 가중치|
|---|---|---|---|
|love|많이|거의 없음|+3.0|
|bad|거의 없음|자주 등장|-2.5|
|product|양쪽에서 비슷하게|비슷하게|0.0|

---
```python
# 예측 및 평가
predictions = model.predict(X_test)
print("Accuracy:", accuracy_score(y_test, predictions))
```

결과:
```python
Accuracy: 0.5
```
결과는 모델의 예측이 50% 맞았다는 뜻입니다.

---
###### 각 용어 설명
| 용어                 | 의미                              |
| ------------------ | ------------------------------- |
| `X_test`           | 테스트용 입력 데이터 (모델이 본 적 없는 문장 벡터들) |
| `y_test`           | 해당 문장들의 정답 감정 (1 = 긍정, 0 = 부정)  |
| `predictions`      | 모델이 예측한 감정 (1 또는 0)             |
| `accuracy_score()` | 예측값과 정답이 얼마나 일치했는지 정확도(%) 계산    |

정확도(Accuracy)란?
	전체 예측 중에서 정답과 일치한 비율
공식:
```
Accuracy = 맞춘 개수 / 전체 개수
```

|문장|실제 감정 (`y_test`)|예측 (`predictions`)|정답 여부|
|---|---|---|---|
|A|1 (긍정)|0 (부정)|❌ 틀림|
|B|0|0|✅ 맞음|
|C|1|1|✅ 맞음|
|D|0|1|❌ 틀림|
총 4개 중 2개 맞음 → `Accuracy = 2 / 4 = 0.5`

`Accuracy: 0.5`의 의미
- 모델이 절반만 맞췄다는 뜻
- 즉, 감정이 긍정인지 부정인지 맞춘 비율이 50%
- 무작위로 찍어도 50%일 수 있기 때문에,  
    성능이 아주 좋다고 보기는 어렵습니다.

왜 정확도가 낮을 수도 있을까?
1. 데이터가 너무 적거나 불균형
2. 단어 선택이 부족하거나 잘못됨
3. 모델이 학습을 충분히 하지 못함
4. 너무 단순한 모델 (예: Naive Bayes)

감정모델의 성능지표를 올리려면 더 많은 감정 문장의 데이터와 그 문장에 맞는 라벨링 데이터가 추가적으로 더 필요합니다.

---
주어진 한국어 문장들에서 가장 자주 등장하는 명사(단어)를 추출하고, 등장 횟수 순으로 정렬해서 상위 5개를 보여주는 코드

전체 목적:
	여러 문장(뉴스 기사 등)에서 자주 나오는 핵심 명사(키워드) 를 뽑아서,  
	어떤 단어들이 이 글의 주요 주제인지 파악하는 데 사용합니다.

---
이것을 허깅페이스 감정분석 파이프라인으로 구현해 보겠습니다.
```python
from sklearn.base import BaseEstimator, ClassifierMixin
from transformers import pipeline

# 1. Hugging Face 감정 분석 파이프라인 래퍼 클래스 정의
class HuggingFaceSentimentClassifier(BaseEstimator, ClassifierMixin):
    def __init__(self):
        self.model = pipeline("sentiment-analysis")

    def predict(self, X):
        results = self.model(X)
        return [r['label'] for r in results]

    def predict_proba(self, X):
        results = self.model(X)
        return [
            [r['score'], 1 - r['score']] if r['label'] == 'POSITIVE'
            else [1 - r['score'], r['score']]
            for r in results
        ]

# 2. 모델 생성
clf = HuggingFaceSentimentClassifier()

# 3. 테스트 데이터
texts = [
    "I love this product!",
    "This is terrible and I hate it.",
    "It's okay, not bad.",
]

# 4. 예측 결과
labels = clf.predict(texts)
probas = clf.predict_proba(texts)

# 5. 출력
for text, label, prob in zip(texts, labels, probas):
    print(f"[입력] {text}")
    print(f"👉 감정: {label}, 확률: {prob}")
    print()
```

```bash
[입력] I love this product!
👉 감정: POSITIVE, 확률: [0.9998855590820312, 0.00011444091796875]

[입력] This is terrible and I hate it.
👉 감정: NEGATIVE, 확률: [0.0005279183387756348, 0.9994720816612244]

[입력] It’s okay, not bad.
👉 감정: POSITIVE, 확률: [0.9997542500495911, 0.00024574995040893555]
```

---
이것을 허깅페이스의 Transformers라이브러리에서 제공하는 사전학습된 딥러닝 모델을 사용하여 아주 쉽게 구현해 볼수 있습니다.

###### 감정분석기 최소 구성 필수 라이브러리
|라이브러리|역할|설치 명령어|
|---|---|---|
|`streamlit`|웹 UI 프레임워크|`pip install streamlit`|
|`transformers`|Hugging Face 모델 불러오기|`pip install transformers`|
|`torch`|딥러닝 모델 실행 (BERT 등)|`pip install torch`|
설치 명령어 한줄로 정리
```bash
pip install streamlit transformers torch
```
이 명령어 하나면 감정 분석기 실행에 필요한 모든 라이브러리가 설치됩니다.

```python
import streamlit as st
import torch

from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, set_seed

# st.title("안녕하세요!")
# st.write("이건 Streamlit으로 만든 웹앱입니다.")

# 감정 분석 파이프라인 - CPU 강제 지정
classifier = pipeline("sentiment-analysis", device=-1)
st.title("😊 영어 감정 분석기")
text = st.text_input("✍️ 문장을 입력하세요 (영어):", "I love using Streamlit!")

if text:
    result = classifier(text)[0]
# text를 감정 분석기에 넣고 나온 결과 중 첫 번째 분석 결과만 result라는 변수에 저장한 것입니다.

    label = result['label']
    score = round(result['score'], 3)

    st.write(f"🔍 감정 예측: **{label}**")
    st.write(f"📊 신뢰도: **{score}**")
```

실행
```bash
streamlit run my_app2.py
```

---
필요한 모듈 불러오기:
```python
from konlpy.tag import Okt
from collections import Counter
```
- `Okt`: 한국어 형태소 분석기 (단어를 쪼개고 품사를 분석해줌)
- `Counter`: 단어의 빈도(횟수) 를 쉽게 세어주는 도구

---
제외하고 싶은 단어
```python
# 불용어(제외할 단어 목록)
stopwords = ['것', '준', '수', '때', '문제', '있다', '하기', '등', '더', '지금', '에바']
```
---
분석 대상 문장 목록
```python
# 샘플 데이터
text_data = [
"미국의 중앙은행인 연방준비제도(Fed·연준)가 지난달 31일(현지 시각) 기준금리를 4연속 동결한 가운데 크리스탈리나 게오르기에바 국제통화기금(IMF) 총재가 “연준 등 주요국 중앙은행이 기준금리를 일찍 내리는 것보다 다소 늦게 내리는 것이 낫다”는 입장을 내놓았다. 1일 블룸버그에 따르면 게오르기에바 총재는 이날 기자 브리핑에서 “중앙은행은 시장의 과도한 기대가 아닌 데이터에 따라 움직여야 한다”며 “지금 시점에선 통화정책이 조기에 완화할 위험이 있다”며 이같이 말했다. 시장에선 연준이 기준금리를 인하할 것이라는 전망이 우세하지만, 연준이 금리를 동결한 것을 긍정적으로 해석한 발언으로 풀이된다.",
"블룸버그는 “게오르기에바 총재의 발언은 제롬 파월 연준 의장이 이르면 다음 달 기준금리를 인하할 것이라는 시장의 기대를 낮추려고 노력하고 있다는 신호를 보낸 지 하루 만에 나왔다”고 짚었다. 연준은 지난달 31일, 기준금리를 5.25~5.50%로 재차 동결했다. 지난해 9월, 11월, 12월에 이어 4번째 동결이다.", 
"시장에선 연준이 이르면 3월부터 금리를 인하하고, 올해 최대 6~7차례에 걸쳐 금리를 인하할 것이라고 전망한다. 연준은 지난해 12월 FOMC 정례회의 직후 발표한 전망에서 올해 말 기준금리가 현재보다 0.65~0.90%포인트(P) 낮은 4.6%에 이를 것으로 예측한 바 있기에 시장에선 최소 3차례 금리를 인하할 것으로 본다. 하지만 파월 연준 의장은 지난달 31일 회의 직후 가진 기자회견에서 “”올해 어느 시점에서 긴축 정책을 완화하는 일을 시작하는 것이 적절할 것”이라면서도 3월에 금리를 인하할 것이란 전망에 대해선 “금리 인하를 보증할 수준의 확신을 얻을 것으로 생각하지 않는다”며 부정적인 입장을 내놓았다.",
"이와 관련 게오르기에바 총재는 연준의 금리 인하 시점에 대해 “몇 달이 걸릴 것이냐의 문제”라며 “금리 인하가 약간 늦는 것보다 조기 완화로 인한 위험이 더 높다고 본다”고 말했다. 이어 “너무 빨리 금리를 인하하면 물가가 안정될 것이라는 소비자나 투자자의 신뢰가 떨어지고, 지금까지 취해진 인플레이션 하락이 역전될 수 있다”고 말했다.",
"다만, 게오르기에바 총재는 높은 금리를 너무 오래 유지할 경우 “미국 경제가 둔화하고, 달러에 다소 부정적인 영향을 미쳐 신흥 시장에 해를 끼칠 위험이 너무 높다”고 경고했다. 그러면서 “필요가 없을 때는 꽉 붙잡고 있지 말아야 한다”며 “지표를 보고 지표에 따라 행동해야 한다”고 말했다.",
]
```
- 뉴스 기사 5개 정도 되는 긴 문장이 리스트에 담겨 있어요.
- 금리, 연준, 인하, 시장, 통화정책 등 경제 관련 주제
---
형태소 분석기 생성
```python
# Okt 객체 생성
okt = Okt()
```
- 형태소 분석기 객체를 만들어줍니다. 이걸로 문장을 분석할 수 있어요.
---
명사만 추출
```python
# 명사 추출
nouns = []
for sentence in text_data:
    for noun in okt.nouns(sentence):
        if noun not in stopwords and len(noun) > 1:
            nouns.append(noun)
```
- 각 문장에서 `okt.nouns()`를 써서 명사만 골라냅니다
- 그리고 `nouns` 리스트에 다 넣어요.
예시:
```python
okt.nouns("기준금리를 동결했다") → ['기준금리', '동결']
```

---
가장 많이 나온 명사 찾기
```python
# 가장 빈번한 명사 찾기
noun_counter = Counter(nouns)
most_common_nouns = noun_counter.most_common(5)
```
- `Counter`로 명사의 등장 횟수를 세고,
- `most_common(5)`를 써서 **상위 5개** 명사와 빈도를 출력합니다.

---
결과 출력
```python
print("Most common nouns:", most_common_nouns)
```

출력결과:
```python
Most common nouns: [('인하', 10), ('금리', 10), ('기준금리', 6), ('시장', 6), ('총재', 5)]
```

Hugging Face의 소스를 가져다 쓰는 것만으로는 "내가 만들고 싶은 걸 진짜 만들었다"고 하기는 어렵습니다. 원리와 구조를 이해해야, 크롤링한 데이터든, 내 서비스에 필요한 기능이든 "내 목적에 맞게 커스터마이징" 할 수 있으니까요.

관심이 가는 분야가 있다면 이렇게 단계별로 접근해 봅니다. (예시 추천예측모델)
###### 1단계: 기본기 다지기 (Python + 기초 ML + Pandas)
| 주제                      | 이유                                    |
| ----------------------- | ------------------------------------- |
| Python 문법 (함수, 클래스, 모듈) | 라이브러리 활용 및 코딩 기본기                     |
| Pandas / NumPy          | 크롤링한 데이터를 다룰 수 있어야 함                  |
| Scikit-learn 기초         | 머신러닝 분류/예측 흐름 이해 (fit → predict 구조 등) |
| 토큰화, 불용어 제거             | 텍스트 데이터 전처리 이해                        |

###### 2단계: Hugging Face와 Transformers 이해
| 주제                         | 이유                                      |
| -------------------------- | --------------------------------------- |
| 🤗 Transformers 개념         | BERT, GPT 등 사전학습 모델이 어떻게 작동하는지 개념적으로 이해 |
| `pipeline()` 구조 이해         | 파이프라인으로 감정 분석하는 방식 습득                   |
| 직접 모델 불러와 토크나이징 → 추론 흐름 이해 | 나중에 커스터마이징 할 수 있게 됨                     |
목표 예제: Hugging Face에서 BERT 불러와 직접 전처리 후 `predict`까지 수행해보기

###### 3단계: 프로젝트 기반 실습 (추천 시스템 or 웹 앱)
| 주제                          | 이유                         |
| --------------------------- | -------------------------- |
| 리뷰 기반 감정 분석 → 점수화           | 긍정도 점수를 매기고 평점 대신 사용       |
| 맛집 추천 알고리즘                  | 점수 기반 sorting / 클러스터링 / 랭킹 |
| 웹앱으로 시각화 (Streamlit 등)      | 유저가 결과를 직접 볼 수 있어야 함       |
| Hugging Face 모델 fine-tuning | 내 데이터로 모델을 미세 조정 가능        |

이런방식으로 모든 인공지능을 접근하신다면 효율적인 방식으로 좋은 결과를 얻어 낼수 있습니다.


```python
import random
from transformers import pipeline

# 파이프라인 설정
en_sentiment = pipeline("sentiment-analysis")
ko_model = "monologg/koelectra-base-v3-discriminator"
ko_sentiment = pipeline("sentiment-analysis", model=ko_model, tokenizer=ko_model)

# 감정 이모지
EMOJI = {
    "POSITIVE": "😄", "NEGATIVE": "😢",
    "LABEL_1": "😢", "LABEL_0": "😄"
}

# 감정에 따른 챗봇 응답
RESPONSE = {
    "POSITIVE": [
        "정말 기분 좋은 일이네요! ✨",
        "그렇게 생각해 주셔서 저도 행복해요!",
        "긍정적인 에너지가 느껴져요 😊",
        "오늘 같은 날엔 산책 나가면 더 기분 좋아질 거예요!",
        "멋지네요! 더 자주 이런 이야기를 나눠요 😊",
        "당신의 하루가 즐겁다니 저도 기뻐요 🎈"
    ],
    "NEGATIVE": [
        "무슨 일이 있었던 거예요? 괜찮으신가요? 😢",
        "저도 마음이 아프네요… 조금 쉬어보는 건 어때요?",
        "힘들었겠어요. 같이 이야기해요.",
        "그런 날도 있죠... 너무 자책하지 마세요 🕊️",
        "오늘은 좀 조용히 쉬는 게 좋을 것 같아요.",
        "제가 곁에 있어줄게요. 편하게 얘기해 주세요."
    ],
    "LABEL_1": [
        "힘들었죠? 괜찮아요, 제가 같이 있어드릴게요 🤗",
        "위로가 되었으면 해요. 괜찮아질 거예요.",
        "마음이 아픈 하루였겠네요. 제 이야기도 들어보실래요?",
        "지금 이 순간, 당신은 혼자가 아니에요."
    ],
    "LABEL_0": [
        "좋은 하루였나 보네요! 🌈",
        "기분 좋아보여서 저도 기뻐요!",
        "오늘은 특별한 일이 있었던 걸까요? 😊",
        "당신의 웃음이 참 보기 좋아요.",
        "좋은 기운이 느껴져요! 감사합니다."
    ]
}

def analyze_and_respond(text: str, lang: str = "ko"):
    pipe = en_sentiment if lang == "en" else ko_sentiment
    result = pipe(text)[0]
    label = result["label"]
    score = result["score"]
    emoji = EMOJI.get(label, "")
    response = random.choice(RESPONSE.get(label, ["잘 이해하지 못했어요. 다시 말씀해주실래요?"]))
    
    return (
        f"{emoji} 감정: {label} ({score:.2f})\n"
        f"🗣 입력: “{text}”\n"
        f"🤖 응답: {response}"
    )

if __name__ == "__main__":
    print("감정 분석 챗봇과 대화해보세요! (종료: exit)\n")
    while True:
        user_input = input("🙂 나: ")
        if user_input.strip().lower() == "exit":
            print("👋 다음에 또 만나요!")
            break
        print(analyze_and_respond(user_input, lang="ko"))
        print("-" * 40)
```

text = "오늘 너무 힘들었어" 라는 글을 입력하면 
Hugging Face 모델로 감정 분석
```python
result = pipeline("sentiment-analysis")(text)
# → result = {'label': 'NEGATIVE', 'score': 0.97}
```

감정 레이블을 기준으로 응답 딕셔너리에서 문장 하나 선택:
```python
response = random.choice(RESPONSE["NEGATIVE"])
# → "무슨 일이 있었던 거예요? 괜찮으신가요? 😢"
```
이런 방식으로 응답합니다.

감정을 더 깊이 있게 다루는 5가지 간단한 방법
1️⃣ 확신도(score)에 따라 말투 조절하기
```python
if score > 0.9:
    tone = "확신"  # 아주 명확한 감정
elif score > 0.7:
    tone = "중간"
else:
    tone = "조심스러운 말투"
```
감정이 애매할수록 말투를 부드럽게 조절할 수 있어요.
```python
if tone == "조심스러운 말투":
    response = f"혹시... {response} 그렇게 느끼신 걸까요?"
```

2️⃣ 사용자 문장 키워드 기반 감정 보정
- "화나", "짜증", "너무 행복", "최고" 등 강한 감정 표현이 있는 경우, `label`과 상관없이 감정 강조
```python
if "너무 행복" in text or "기분 최고" in text:
    label = "POSITIVE"
elif "짜증" in text or "화나" in text:
    label = "NEGATIVE"
```

3️⃣ 시간 기반 감정 차등화 (아침/밤 말투 변화)
```python
from datetime import datetime

hour = datetime.now().hour
if hour >= 22 or hour < 6:
    greeting = "늦은 시간이네요. "
else:
    greeting = "좋은 하루 보내고 계신가요? "

# 응답에 greeting 추가
return greeting + full_response
```

4️⃣ 이전 대화 맥락을 기억하여 연속 반응 (간단히)
```python
# 전역변수로 이전 감정 기억
prev_label = None

def analyze_and_respond(text, lang="ko"):
    global prev_label
    ...
    if prev_label == "LABEL_0" and label == "LABEL_1":
        response += " 방금보다 더 좋아지신 것 같네요!"
    prev_label = label
```

5️⃣ 감정 점수 기반 ‘진폭’ 표현
```python
if score > 0.95:
    response = f"정말 정말 그렇게 느끼셨군요! {response}"
elif score < 0.6:
    response = f"혹시 제 오해일 수도 있지만... {response}"
```


-----
```bash
pip install pandas torch konlpy
```

1 기본 라이브러리 임포트 & 하이퍼파라미터
```python
import os
import pandas as pd
import itertools
from collections import Counter

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader

from konlpy.tag import Okt

# 하이퍼파라미터
MAX_LEN       = 100     # 패딩/트렁케이트 길이
BATCH_SIZE    = 64
EMBED_DIM     = 128
HIDDEN_DIM    = 128
OUTPUT_DIM    = 1       # 이진 분류
N_LAYERS      = 2
DROPOUT       = 0.5
NUM_EPOCHS    = 5
LEARNING_RATE = 1e-3

DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print("Using device:", DEVICE)
```

2 데이터프레임 로딩
```python
train_df = pd.read_csv("data/train_data.csv")
test_df  = pd.read_csv("data/test_data.csv")

print(f"▶ train: {len(train_df)} rows, test: {len(test_df)} rows")
train_df.head(3)
```

3 토크나이저 & Vocabulary 구성
```python
from konlpy.tag import Okt
from collections import Counter
import itertools

okt = Okt()

# 1) 토큰화: train_df["text"] → 각 문장별 토큰 리스트 시리즈
train_tokens = train_df["text"].map(okt.morphs)

# 2) 모든 토큰을 평평하게(flatten) 펼쳐서 빈도 계산
#    itertools.chain.from_iterable(리스트의 리스트) 를 사용
all_tokens = itertools.chain.from_iterable(train_tokens)
counter    = Counter(all_tokens)

# 3) 최소 빈도 2 이상인 토큰만 필터링
filtered = [tok for tok, cnt in counter.items() if cnt >= 2]

# 4) 인덱스 부여: 0=<pad>, 1=<unk>, 그 외 2부터
vocab = {tok: idx+2 for idx, tok in enumerate(filtered)}
vocab["<pad>"] = 0
vocab["<unk>"] = 1

# 5) 전체 단어 수
VOCAB_SIZE = len(vocab)

print(f"▶ vocab size: {VOCAB_SIZE}")
print("▶ sample tokens:", list(vocab.items())[:10])
```

4 PyTorch Dataset & DataLoader
```python
class MovieReviewDataset(Dataset):
    def __init__(self, df, vocab, tokenizer, max_len=MAX_LEN):
        self.texts     = df["text"].tolist()
        self.labels    = df["label"].astype(float).tolist()
        self.vocab     = vocab
        self.tokenizer = tokenizer
        self.max_len   = max_len

    def __len__(self):
        return len(self.texts)
    
    def numericalize(self, tokens):
        ids = [ self.vocab.get(t, self.vocab["<unk>"]) for t in tokens ]
        if len(ids) < self.max_len:
            ids += [ self.vocab["<pad>"] ] * (self.max_len - len(ids))
        else:
            ids = ids[:self.max_len]
        return ids

    def __getitem__(self, idx):
        tokens = self.tokenizer(self.texts[idx])
        ids    = self.numericalize(tokens)
        label  = self.labels[idx]
        return torch.tensor(ids,   dtype=torch.long), \
               torch.tensor(label, dtype=torch.float)

# 인스턴스 생성
train_ds = MovieReviewDataset(train_df, vocab, okt.morphs)
test_ds  = MovieReviewDataset(test_df,  vocab, okt.morphs)

# Collate 함수
def collate_fn(batch):
    texts, labels = zip(*batch)
    return torch.stack(texts), torch.stack(labels)

# DataLoader
train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,  collate_fn=collate_fn)
test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)

print("▶ train batches:", len(train_loader), " test batches:", len(test_loader))
```

5 LSTM 기반 분류 모델 정의
```python
import torch.nn as nn

class SentimentLSTM(nn.Module):
    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim,
                 n_layers, dropout):
        super().__init__()
        # vocab_size 를 VOCAB_SIZE 로 맞춤
        self.embedding = nn.Embedding(
            num_embeddings=vocab_size,
            embedding_dim=embed_dim,
            padding_idx=vocab["<pad>"]  # padding index 지정
        )
        self.lstm      = nn.LSTM(
            embed_dim, hidden_dim,
            num_layers=n_layers,
            batch_first=True,
            dropout=dropout if n_layers>1 else 0
        )
        self.fc        = nn.Linear(hidden_dim, output_dim)
        self.dropout   = nn.Dropout(dropout)
        
    def forward(self, x):
        # x: [batch, seq_len]
        embedded = self.dropout(self.embedding(x))
        _, (hn, _) = self.lstm(embedded)
        last_hidden = hn[-1]            # [batch, hidden_dim]
        return self.fc(self.dropout(last_hidden)).squeeze(1)

# 모델 생성 시 VOCAB_SIZE 사용
model = SentimentLSTM(
    vocab_size=VOCAB_SIZE,
    embed_dim=EMBED_DIM,
    hidden_dim=HIDDEN_DIM,
    output_dim=OUTPUT_DIM,
    n_layers=N_LAYERS,
    dropout=DROPOUT
).to(DEVICE)
```

6 학습 및 평가 함수
```python
def train_epoch(model, loader, crit, opt):
    model.train()
    total_loss = 0
    for texts, labels in loader:
        texts, labels = texts.to(DEVICE), labels.to(DEVICE)
        opt.zero_grad()
        preds = model(texts)
        loss  = crit(preds, labels)
        loss.backward()
        opt.step()
        total_loss += loss.item()
    return total_loss / len(loader)

def evaluate(model, loader, crit):
    model.eval()
    total_loss, correct, total = 0, 0, 0
    with torch.no_grad():
        for texts, labels in loader:
            texts, labels = texts.to(DEVICE), labels.to(DEVICE)
            preds = torch.sigmoid(model(texts))
            loss  = crit(preds, labels)
            total_loss += loss.item()
            # 이진 정확도
            predicted = (preds >= 0.5).float()
            correct += (predicted == labels).sum().item()
            total   += labels.size(0)
    return total_loss / len(loader), correct/total
```

7 전체 학습 & 테스트 실행
```python
for epoch in range(1, NUM_EPOCHS+1):
    train_loss = train_epoch(model, train_loader, criterion, optimizer)
    val_loss, val_acc = evaluate(model, test_loader, criterion)
    print(f"Epoch {epoch:02d} ▶ Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}")
```
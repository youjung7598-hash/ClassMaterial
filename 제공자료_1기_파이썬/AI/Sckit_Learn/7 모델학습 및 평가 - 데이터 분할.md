`1.` 데이터 분할이란?
하나의 데이터셋을 여러 용도로 나누는 과정
용도별 구분
    - 학습용(Training): 모델을 훈련시키는 데 사용
    - 검증용(Validation): 훈련 중에 성능을 점검하는 데이터
    - 테스트용(Test): 최종적으로 모델 성능을 객관적으로 평가하는 데이터
        
한 번에 모든 데이터를 쓰면, 모델이 학습 데이터에만 너무 잘 맞고(=과적합), 새로운 데이터에는 성능이 떨어질 수 있어요.

---
`2.` 데이터 분할의 이유 (중요성)
과적합 방지
- 학습 데이터에만 너무 최적화되는 걸 막아 줘요.
    
신뢰성 있는 평가
- 학습에 사용되지 않은 테스트 데이터로 평가해야, 실제 환경에서도 잘 작동할지 알 수 있어요.
---
`3.` 데이터 분할 방법
랜덤 분할 (Random Split)
- 데이터를 무작위로 섞어서 학습/검증/테스트용으로 나눔
- 가장 기본적인 방법
```python
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split

# 1) 데이터 불러오기
bc = load_breast_cancer()
X, y = bc.data, bc.target

# 2) 20%를 테스트용으로 떼어내고, 나머지 80%는 학습용으로 사용
X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,       # 테스트 데이터 비율 (20%)
    random_state=42      # 결과를 재현하기 위한 시드 값
)

# 3) 분할된 데이터 크기 확인
print("학습 데이터 개수:", X_train.shape[0])
print("테스트 데이터 개수:", X_test.shape[0])
```
---
계층적 분할 (Stratified Split)
- 특징: 원본 데이터의 클래스 비율(예: 긍정/부정)이 학습/테스트에도 비슷하게 유지돼요.
- 언제 쓰나요?: 분류 문제에서, 데이터의 클래스 분포가 분할 후에도 균형을 맞춰야 할 때.
```python
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import StratifiedShuffleSplit

# 1) 데이터 불러오기
cancer = load_breast_cancer()
X, y = cancer.data, cancer.target

# 2) StratifiedShuffleSplit 객체 생성
strat_split = StratifiedShuffleSplit(
    n_splits=1,      # 몇 번 나눌지
    test_size=0.2,   # 테스트 비율 (20%)
    random_state=42
)

# 3) 실제 분할 실행
for train_idx, test_idx in strat_split.split(X, y):
    X_train_strat = X[train_idx]
    X_test_strat  = X[test_idx]
    y_train_strat = y[train_idx]
    y_test_strat  = y[test_idx]

# 4) 클래스 비율 확인
mal_ratio_train = sum(y_train_strat == 0) / len(y_train_strat)
mal_ratio_test  = sum(y_test_strat  == 0) / len(y_test_strat)
print("학습 데이터에서 악성 비율:", mal_ratio_train)
print("테스트 데이터에서 악성 비율:", mal_ratio_test)
```
---
시계열 분할 (Time Series Split)
- 특징: 시간 순서대로 과거→미래 순으로 나눠요.
- 언제 쓰나요?: 주식 예측, 날씨 예측처럼 시간 정보가 중요한 경우.
- 코드 예제
```python
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import TimeSeriesSplit

# 1) 데이터 불러오기 (여기서는 분류 예제로 씁니다)
cancer = load_breast_cancer()
X, y = cancer.data, cancer.target

# 2) TimeSeriesSplit 객체 생성
tscv = TimeSeriesSplit(n_splits=5)

# 3) 분할 실행
for train_idx, test_idx in tscv.split(X):
    X_train_time = X[train_idx]
    X_test_time = X[test_idx]
    y_train_time = y[train_idx]
    y_test_time = y[test_idx]

# 4) 분할된 마지막 조각 크기 확인
print("시계열 학습 데이터 개수:", X_train_time.shape[0])
print("시계열 테스트 데이터 개수:", X_test_time.shape[0])
```
---
```python
# === 랜덤 데이터 분할 (Random Split) 예제 ===
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split

# 1) 데이터 불러오기
# load_breast_cancer()는 유방암 진단용 데이터셋을 가져옵니다.
bc = load_breast_cancer()
X, y = bc.data, bc.target  # X: 특성(입력값), y: 레이블(정답)

# 2) train_test_split 함수로 무작위 분할
# test_size=0.2  → 전체 중 20%를 테스트용으로 떼어냅니다.
# random_state=42 → 결과를 재현할 수 있게 난수 시드를 고정합니다.
X_train_rand, X_test_rand, y_train_rand, y_test_rand = train_test_split(
    X, y,
    test_size=0.2,
    random_state=42
)

# 3) 분할된 데이터 개수 출력
print("Random Split:")
print("  Train set size:", X_train_rand.shape[0], "개") 
# 학습용 샘플 개수

print("  Test set size: ", X_test_rand.shape[0], "개")  
# 테스트용 샘플 개수
```

```python
# === 계층적 데이터 분할 (Stratified Split) 예제: 크기 확인 ===
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import StratifiedShuffleSplit

# 1) 데이터 불러오기
cancer = load_breast_cancer()
X, y = cancer.data, cancer.target

# 2) StratifiedShuffleSplit 객체 설정
# n_splits=1     → 분할을 1번 수행
# test_size=0.2  → 20%를 테스트용으로 할당
# random_state=42 → 결과 재현용 시드 고정
stratified_split = StratifiedShuffleSplit(
    n_splits=1,
    test_size=0.2,
    random_state=42
)

# 3) split() 메서드로 인덱스 생성 → 학습/테스트 인덱스 분리
for train_index, test_index in stratified_split.split(X, y):
    X_train_strat = X[train_index]
    X_test_strat  = X[test_index]
    y_train_strat = y[train_index]
    y_test_strat  = y[test_index]

# 4) 분할된 데이터 개수 출력
print("\nStratified Split:")
print("  Train set size:", X_train_strat.shape[0], "개")  
# 학습용 샘플 개수

print("  Test set size: ", X_test_strat.shape[0], "개")   
# 테스트용 샘플 개수
```

```python
# === 시계열 데이터 분할 (Time Series Split) 예제 ===
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import TimeSeriesSplit

# 1) 데이터 불러오기 (여기서는 분류 예제로 사용)
cancer = load_breast_cancer()
X, y = cancer.data, cancer.target

# 2) TimeSeriesSplit 객체 생성
#   n_splits=5 → 5개의 순차적인 훈련/테스트 조합 생성
tscv = TimeSeriesSplit(n_splits=5)

# 3) split() 메서드로 시간 순서에 따라 분할
for fold, (train_index, test_index) in enumerate(tscv.split(X), start=1):
    X_train_time = X[train_index]
    X_test_time = X[test_index]
    y_train_time = y[train_index]
    y_test_time = y[test_index]
    print(f"\nFold {fold}:")
    print("  Train set size:", X_train_time.shape[0], "개")
    print("  Test set size: ", X_test_time.shape[0], "개")

# → 과거 데이터를 먼저 학습하고, 이후의 데이터를 테스트하는 순서가 유지돼요.
```

---
공공데이터처럼 용량이 크고 양이 방대한 데이터셋을 일부만 사용해 테스트하고 싶을 때는, 무작정 줄이는 것보다 목적에 맞는 분할 전략이 필요합니다.

데이터 분할 전략
`1.`샘플링 기반 분할
- 의도: 전체 중 일부 샘플만 무작위로 추출해 테스트
방법
```python
import random
import os
data_dir = "dataset/videos"  # 영상 폴더
all_files = os.listdir(data_dir)
sampled_files = random.sample(all_files, 100)  # 100개만 선택
```
추천 상황: 빠르게 모델 테스트하거나 라벨 확인용으로 일부만 확인할 때

`2.`클래스 기준 필터링 (라벨 기준 분할)
- 의도: 특정 영상 또는 텍스트 단어 또는 범주만 선택
방법
```python
import pandas as pd
df = pd.read_csv("labels.csv")
subset_df = df[df["label"].isin(["안녕하세요", "감사합니다"])]  
# 수어나 텍스트 단어로 필터
```
추천 상황: 특정 동작(단어)만 학습하거나 분류 성능을 세밀하게 테스트할 때

`3.`시간/프레임 수 기준 필터링
- 의도: 너무 긴 영상 제외 → 학습 속도 개선
방법
```python
import cv2
short_videos = []
for file in os.listdir("dataset/videos"):
    cap = cv2.VideoCapture(f"dataset/videos/{file}")
    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    if frame_count < 100:  # 프레임 수가 100 미만인 경우만 사용
        short_videos.append(file)
    cap.release()
```
추천 상황: 프레임 기반 시퀀스 처리 모델(LSTM, CNN+RNN) 테스트 시

`4.`파일명 기준으로 필터링 (규칙 기반)
- 의도: 예를 들어 `0001_hello.mp4`, `0002_thankyou.mp4`처럼 규칙이 있다면
방법
```python
files = [f for f in os.listdir("dataset/videos") if "hello" in f]
```
추천 상황: 사전 명명 규칙이 잘 정해진 경우 빠른 필터링 가능

`5.`train / valid / test 분할 자동화
- 의도: 전체에서 일정 비율로 나누어 실험에 적합한 구조 생성
방법
```python
from sklearn.model_selection import train_test_split

files = os.listdir("dataset/videos")
train, temp = train_test_split(files, test_size=0.4, random_state=42)
val, test = train_test_split(temp, test_size=0.5, random_state=42)

print(f"Train: {len(train)}, Val: {len(val)}, Test: {len(test)}")
```
추천 상황: 정식 모델 학습을 위해 전체 데이터를 일정 비율로 나누고 싶을 때

저장 폴더 구조 구성
```bash
dataset/
├── train/
│   ├── 안녕하세요_001.mp4
│   └── 감사합니다_002.mp4
├── val/
├── test/
└── labels.csv
```
이런 구조는 PyTorch, TensorFlow 등 프레임워크에서 `ImageFolder`, `VideoDataset`, `DataLoader`를 사용할 때 매우 유용합니다.

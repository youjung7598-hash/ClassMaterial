머신러닝(Machine Learning)은 컴퓨터가 스스로 배워서 문제를 해결하는 기술입니다.  크게 3가지 종류로 나눌 수 있어요:
1. 지도 학습 (Supervised Learning)
2. 비지도 학습 (Unsupervised Learning)
3. 강화 학습 (Reinforcement Learning)


그러나 100% 스스로 하는 것은 아닙니다.
사람이 해야 하는 것들
1. 학습할 데이터를 준비
2. 신경망 **구조** 설계 (레이어 수, 활성화 함수 등)
3. 손실함수, 최적화 알고리즘 선택
4. 하이퍼파라미터 조정 (학습률, 배치 크기 등)

---
`1.` 지도 학습 (Supervised Learning)

✅ 쉽게 말하면?
- 정답을 알려주면서 배우는 학습입니다.
- 예를 들어, “이 이메일은 스팸이다”라는 정답(레이블)이 주어져 있고, 컴퓨터는 이 정보를 바탕으로 새로운 이메일이 스팸인지 아닌지를 예측할 수 있게 됩니다.
    
✅ 주요 특징
- 학습할 때 입력 데이터와 정답(레이블)이 같이 제공됩니다.
- 분류(Classification) 또는 회귀(Regression) 문제를 해결합니다.
    
✅ 예시
- 이메일 스팸 분류 (이메일 → 스팸/스팸 아님)
- 집값 예측 (집의 조건들 → 예상 가격)

입력(X) 과 정답(y, 레이블) 이 함께 제공되면 지도학습입니다.
이전에 학습했던 아이리스 꽃 분류와 손글씨 숫자 예측도 지도학습이였습니다.
```python
# 필요한 도구(라이브러리)불러오기
from sklearn.datasets import load_digits
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# 1. 손글씨 숫자 데이터 불러오기
digits = load_digits()
X = digits.data     # 손글씨 이미지(숫자 0~9)의 픽셀 정보
y = digits.target   # 그 이미지가 어떤 숫자인지 정답(label)

# 2. 훈련용과 테스트용 데이터 나누기
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# - 전체 데이터 중 80%는 훈련용, 20%는 테스트용  
# - random_state=42: 결과가 항상 똑같이 나오도록 고정

# 3. 로지스틱 회귀 모델 학습
model = LogisticRegression(max_iter=1000)
model.fit(X_train, y_train)
# - LogisticRegression: 분류에 사용되는 머신러닝 모델  
# - .fit(): 훈련 데이터를 이용해 학습 수행

# 4. 예측 및 정확도 평가
y_pred = model.predict(X_test)
print("정확도:", accuracy_score(y_test, y_pred))
# - predict: 테스트 데이터(X_test)에 대해 숫자를 예측함   
# - accuracy_score: 예측 결과와 실제 정답(y_test)를 비교해 정확도를 계산
```

결과:
```
- 정확도: 0.975 → 테스트 데이터 중 약 97.5%를 정확하게 맞췄다는 뜻!
```

---
`2.` 비지도 학습 (Unsupervised Learning)

✅ 쉽게 말하면?
- 정답 없이 데이터만 주고 스스로 패턴을 찾는 학습입니다.
- 사람도 모르는 데이터를 컴퓨터가 분석해서 "이런 식으로 나눌 수 있겠네!"라고 그룹을 나누는 방식입니다.
    
✅ 주요 특징
- 레이블(정답)이 없습니다.
- 컴퓨터가 스스로 구조, 패턴, 관계를 찾습니다.
- 대표적인 기법은 군집화(Clustering), 차원 축소(Dimensionality Reduction) 등이 있어요.
    
✅ 예시
- 고객을 비슷한 소비패턴으로 묶기 (고객 세분화)
- 시장에서 자주 같이 구매되는 상품 분석 (장바구니 분석)

```python
from sklearn.datasets import load_iris
from sklearn.cluster import KMeans # (K:군집, Means:평균)
import matplotlib.pyplot as plt

# 1. 데이터 로드
iris = load_iris()
X = iris.data # (150, 4)

# 2. KMeans 클러스터링
kmeans = KMeans(n_clusters=3, random_state=42)
kmeans.fit(X)
labels = kmeans.labels_

# 3. 시각화 (2차원으로 축소)
plt.scatter(X[:, 0], X[:, 1], c=labels)
plt.xlabel("꽃받침 길이")
plt.ylabel("꽃받침 너비")
plt.title("KMeans 클러스터링 결과")
plt.show()
```

위의 코드는 KMeans 클러스터링을 사용하여 아이리스 데이터셋을 3개의 군집으로 나누고, 그 결과를 시각화한 것입니다.

코드 분석:
	
1. 데이터 로드: `load_iris()`를 통해 아이리스 데이터셋을 로드합니다. 이 데이터셋은 4개의 특성(꽃받침 길이, 꽃받침 너비, 꽃잎 길이, 꽃잎 너비)과 3개의 클래스(아이리스의 종류)로 구성되어 있습니다.
    
2. KMeans 클러스터링: `KMeans(n_clusters=3)`으로 3개의 군집으로 데이터를 나누는 모델을 만들고, `fit()`을 통해 데이터를 학습합니다. 학습된 모델을 기반으로 `labels_` 속성에서 각 데이터 포인트가 속한 군집을 확인할 수 있습니다.
    
3. 시각화: `plt.scatter()`를 이용하여 꽃받침 길이와 꽃받침 너비를 x, y 축으로 두고, 각 데이터 포인트를 군집별 색깔로 구분하여 시각화합니다. 군집의 색은 `labels`로 지정된 군집 번호에 따라 달라집니다.
	
해석:
	
- 클러스터링 결과: KMeans가 아이리스 데이터셋을 3개의 군집으로 나누었음을 알 수 있습니다. 이 군집들은 아이리스의 실제 종에 상관없이, 꽃받침 길이와 너비를 기준으로 분류된 것입니다.
    
- 군집 시각화: 각 군집은 서로 다른 색으로 구분됩니다. 그래프에서 군집별로 분리된 패턴을 볼 수 있는데, 이는 KMeans가 이 데이터를 어떻게 군집화했는지를 나타냅니다.
    
실제 아이리스 꽃의 종과의 관계:
- KMeans는 비지도 학습이므로 실제 아이리스 꽃의 종을 모르고 군집화한 것입니다. 하지만, 그래프에서 군집을 보면 실제 꽃의 종에 맞춰 군집들이 잘 분리된 것을 확인할 수 있습니다.
    
요약:
	
- 이 그래프는 KMeans 클러스터링이 아이리스 데이터셋에서 3개의 군집을 찾아냈고, 그 군집들은 꽃받침 길이와 너비를 기준으로 잘 구분되었음을 보여줍니다.
	
- 클러스터링 결과가 실제 꽃의 종과 어느 정도 일치할 수 있지만, 클러스터링은 레이블(종)을 알지 못한 상태에서 데이터를 그룹화하는 비지도 학습 방법이라는 점을 기억해야 합니다.

결과:
![[Pasted image 20250728172902.png]]

|항목|값|
|---|---|
|총 데이터 수|150개|
|점의 개수|150개|
|점 하나당 의미|꽃 한 송이|
|색깔|군집화 결과 (0, 1, 2 중 하나)|
|좌표 (x, y)|꽃받침 길이 vs 너비|
웹사이트에 적용하면 사용자에게 개인화된 추천, 분류, 그룹별 분석 등을 제공할 수 있어요.
###### 군집화를 응용하여 웹사이트 제작시 예시:
| 분야   | 예시 웹 기능               | 군집화 응용 방식                                    |
| ---- | --------------------- | -------------------------------------------- |
| 쇼핑몰  | 비슷한 고객끼리 묶어서 맞춤 상품 추천 | 고객의 구매 패턴(가격, 카테고리, 브랜드 등)을 기반으로 클러스터링       |
| 음악 앱 | 유사한 음악 선호 고객끼리 묶기     | 사용자의 청취 기록 기반으로 군집화하여 취향 비슷한 사용자끼리 플레이리스트 추천 |
| 여행   | 여행지 선호 유형 분류          | 사용자의 여행 경로, 관심 테마(자연, 도시 등)를 분석해 유사 그룹 생성    |
| 건강   | 건강 설문 기반 사용자 유형 분류    | 식습관, 운동 습관 등의 데이터로 건강 유형 군집화                 |
| 교육   | 학습 성향 분류 및 맞춤 과제 추천   | 시험 점수, 학습 속도 등으로 학생을 군집화하여 학습 경로 설계          |
| 커뮤니티 | 글/댓글 기반 유저 유형 분석      | 사용자가 자주 쓰는 단어, 관심 주제 등으로 그룹화하여 맞춤 콘텐츠 노출     |
KMeans는 사이킷런(sklearn)의 `sklearn.cluster` 모듈에 포함된 군집 알고리즘 중 하나입니다.  
사이킷런에는 KMeans 말고도 여러 군집화 알고리즘이 있습니다.
- `KMeans` : 평균 기반 군집화
- `DBSCAN` : 밀도 기반 군집화
- `AgglomerativeClustering` : 계층적 군집화

---
`3.` 강화 학습 (Reinforcement Learning)
- 보상을 받으며 시행착오로 배우는 학습입니다.
- 게임처럼 어떤 행동을 하면 보상(점수)을 받고, 그 보상을 최대화하는 방향으로 계속 학습합니다.
    
✅ 예시
- 알파고(바둑 두는 인공지능)
- 자율주행 자동차

| 목적                | 설명                                                      |
| ----------------- | ------------------------------------------------------- |
| 강화학습이란?           | 보상(reward)을 최대화하기 위해 환경과 상호작용하며 학습하는 방법                 |
| Q-Learning이란?     | 각 상태-행동쌍에 대한 가치를 테이블(Q-table)로 저장하면서 최적 전략을 찾는 알고리즘     |
| 탐험과 활용            | `ε-greedy`를 통해 탐험(새로운 시도)과 활용(이미 알고 있는 최선 행동)을 균형 있게 수행 |
| 보상이 없는 초기 상태에서 학습 | 처음에는 아무것도 모르고 무작위로 행동하지만, 반복을 통해 보상 1.0을 얻는 길을 스스로 학습함  |
| 추론 가능한 결과 확인      | 학습이 완료된 후, 테스트를 통해 에이전트가 올바른 경로로 이동하는지 평가 가능            |

```bash
pip install gymnasium
```

강화학습 알고리즘인 Q-Learning을 이용하여  
게임판(FrozenLake)에서 출발점에서 도착점까지 가는 방법을 스스로 학습하는 AI 코드를 작성한 것입니다.
```python
# numpy는 숫자 계산, 배열(Q-테이블) 만들 때 사용    
# gymnasium은 게임 환경(FrozenLake)을 제공하는 라이브러리예요
import numpy as np
import gymnasium as gym
import matplotlib.pyplot as plt


# 1. 환경 생성 (미끄러지지 않도록 설정 → 더 쉽게 성공 가능)
env = gym.make("FrozenLake-v1", is_slippery=False)
# FrozenLake-v1: 얼음 위를 이동하는 4x4 게임판  
# is_slippery=False: 미끄러지지 않도록 설정 → 내가 선택한 방향으로만 이동


# 2. Q-테이블 초기화: 상태 16개 x 행동 4개
q_table = np.zeros((env.observation_space.n, env.action_space.n))
# Q-테이블은 AI가 학습한 내용을 저장하는 표입니다.   
# 상태 수 = 16개 (4x4 격자 → 0~15)   
# 행동 수 = 4개 (왼쪽, 아래, 오른쪽, 위 → 0~3)   
# 전부 0으로 시작


# 3. 하이퍼파라미터 설정
alpha = 0.1         # 학습률 (얼마나 빨리 배울지)
gamma = 0.99        # 할인율 (미래 보상을 얼마나 중요하게 생각할지)
epsilon = 1.0       # 처음에는 무작위 행동을 많이 하자!
epsilon_decay = 0.995  # 학습하면서 조금씩 무작위 행동 줄이기
epsilon_min = 0.01     # 무작위는 최소 1%까지 유지
episodes = 5000        # 학습 반복 횟수


# 4. 학습 반복
for episode in range(episodes):
    state, _ = env.reset()  # 게임 초기화, 시작 위치 받기
    done = False            # 게임이 끝났는지 아닌지 저장


    while not done: # 게임이 끝날 때까지 반복 (목표 도달 or 빠짐)
        # ε-탐욕 정책 (exploration vs exploitation)
        if np.random.rand() < epsilon:
            action = env.action_space.sample() # 무작위 행동
        else:
            action = np.argmax(q_table[state])
            # 지금까지 배운 것 중 최고 행동
			# 처음에는 epsilon=1.0이라 무작위 행동이 많음
		    # 점점 배운 행동 위주로 바뀜
			
			
        # 행동 수행 → 결과 받기
        next_state, reward, terminated, truncated, _ = env.step(action)
        done = terminated or truncated # 도착했거나 빠졌으면 종료

        # Q-값 업데이트 (학습 핵심 부분!)
        q_table[state, action] = q_table[state, action] + alpha * (reward + gamma * np.max(q_table[next_state]) - q_table[state, action])
# 쉽게 말하면:
# 지금 했던 행동이 얼마나 좋았는지 계산해서 Q-테이블에 기록한다
# reward: 지금 받은 보상
# np.max(...): 다음 상태에서 가장 좋은 행동의 Q값
# gamma: 미래 보상을 얼마나 중요하게 볼지
# alpha: 얼마나 빨리 업데이트할지

		# 다음 상태로 이동
        state = next_state

    # 무작위 행동 비율 조금 줄이기
    if epsilon > epsilon_min:
        epsilon *= epsilon_decay
		# epsilon을 매 에피소드마다 조금씩 줄이기
	    # 학습이 될수록 무작위 행동보다 배운 행동을 더 믿게 함

    if (episode + 1) % 100 == 0:
        test_success = 0
        for _ in range(10):
            s, _ = env.reset()
            done = False
            while not done:
                a = np.argmax(q_table[s])
                s_, r, term, trunc, _ = env.step(a)
                done = term or trunc
                s = s_
            if r == 1.0:
                test_success += 1
        success_list.append(test_success)	


# 5. 학습 완료 후 테스트
print("\n 학습 완료! 테스트 시작...\n")

success = 0
# 테스트 10번 반복
for test in range(10):
    state, _ = env.reset()
    done = False
    print(f" 테스트 {test+1} 시작: ", end="")

	# 학습된 Q-table을 기반으로 최선의 행동만 선택
    for step in range(100):
        action = np.argmax(q_table[state])  
        # 학습된 Q값 기준으로 행동 선택


		# 행동하고 결과 출력(보상과 패널티)
        next_state, reward, terminated, truncated, _ = env.step(action)
        done = terminated or truncated
        print(state, "→", end=" ")
        state = next_state
        if done:
            break

	# 최종 보상 확인 (성공했는지)
    print(f"{state} → 보상: {reward}")
    if reward == 1.0:
        success += 1

# 테스트 전체 결과 요약
print(f"\n 테스트 성공 횟수: {success}/10")

# 0번 중 몇 번 성공했는지 출력
# 성공하면 보상: 1.0
```
	이 코드는 AI가 아무 것도 모르는 상태에서  
	반복된 시도와 보상을 통해  
	목표까지 도달하는 최적 경로를 학습한 다음,  
	그 경로를 테스트에서 얼마나 잘 쓰는지를 평가하는 전체 흐름입니다!

결과:
```
학습 완료! 테스트 시작...

테스트 1 시작: 0 → 4 → 8 → 9 → 10 → 14 → 15 →  보상: 1.0
테스트 2 시작: 0 → 4 → 8 → 9 → 10 → 14 → 15 →  보상: 1.0
테스트 3 시작: 0 → 4 → 8 → 9 → 10 → 14 → 15 →  보상: 1.0
테스트 4 시작: 0 → 4 → 8 → 9 → 10 → 14 → 15 →  보상: 1.0
테스트 5 시작: 0 → 4 → 8 → 9 → 10 → 14 → 15 →  보상: 1.0
테스트 6 시작: 0 → 4 → 8 → 9 → 10 → 14 → 15 →  보상: 1.0
테스트 7 시작: 0 → 4 → 8 → 9 → 10 → 14 → 15 →  보상: 1.0
테스트 8 시작: 0 → 4 → 8 → 9 → 10 → 14 → 15 →  보상: 1.0
테스트 9 시작: 0 → 4 → 8 → 9 → 10 → 14 → 15 →  보상: 1.0
테스트 10 시작: 0 → 4 → 8 → 9 → 10 → 14 → 15 →  보상: 1.0
```

4X4 격자 구조
```
 0  1  2  3
 4  5  6  7
 8  9 10 11
12 13 14 15
```
- `0`: 출발점(S)
- `15`: 목표지점(G)
- `H`: 구멍(빠지면 실패)
- `F`: 일반 빙판길

AI가 학습한 이동 경로는?
```
0 → 4 → 8 → 9 → 10 → 14 → 15
```
즉, 출발해서:
- 아래로 한 칸씩 가고
- 오른쪽으로 이동하면서
- 최적 경로를 따라 안전하게 목표에 도달한 것입니다


웹사이트에 Q-Learning을 응용할 수 있는 예시:

1. 개인화 추천 웹사이트
	사용자 행동 로그를 분석하여 최적의 콘텐츠/상품 추천
- 상태 = 사용자 행동 이력 (페이지 체류 시간, 클릭 상품 등)
- 행동 = 추천할 상품 or 콘텐츠
- 보상 = 클릭 여부, 구매 여부
- Q-Learning을 통해 “어떤 사용자에겐 어떤 콘텐츠가 적절한가”를 학습
---
2. 학습 서비스/퀴즈 서비스
	문제 추천 시스템을 강화학습으로 구현
- 상태 = 학생의 현재 실력 수준
- 행동 = 다음에 추천할 문제
- 보상 = 맞혔는지 틀렸는지
- Q-Learning으로 “학생에게 가장 적절한 문제를 자동 추천”
---
3. 스마트 일정 추천 서비스
	예: 미세먼지, 날씨, 중요도에 따라 가장 적절한 시간에 일정 배치
- 상태 = 현재 시간, 일정 유형
- 행동 = 일정 배치 위치
- 보상 = 일정이 성공적으로 끝났는지, 사용자가 만족했는지
- 강화학습을 통해 일정 우선순위와 배치를 최적화
---
4. 채팅 AI/챗봇 웹서비스
	사용자 응답 패턴에 따라 강화학습으로 대화 전략 학습
- 상태 = 대화 흐름 상태
- 행동 = 챗봇의 다음 문장 선택
- 보상 = 사용자 반응 (이탈률 감소, 클릭 증가 등)

사용자는 전혀 눈치 못 채지만,  
강화학습 시스템이 내부적으로
- 클릭·구매 같은 긍정적인 행동 → 보상(Reward)
- 무시·이탈 같은 부정적인 행동 → 패널티(Penalty)  
    로 기록하고, 그 데이터를 이용해 다음 추천 전략을 조정하는 거예요.

예시 – 쇼핑몰 추천 강화학습 흐름
1. 사용자 행동 수집
    - 상품 A 클릭 → `+1 보상`
    - 상품 B 장바구니 → `+3 보상`
    - 상품 C 구매 → `+10 보상`
    - 상품 D 무시 → `-1 패널티`
        
2. 정책(Policy) 업데이트
    - `상품 A`와 비슷한 상품을 더 자주 추천
    - `상품 D`와 관련된 추천은 줄임
        
3. 다음 사용자가 접속했을 때
    - 이전에 보상이 높았던 추천 패턴을 우선 적용

---
날씨/위치/클릭/좋아요 데이터를 통한 개인 취향 파악 추천 알고리즘
“초기엔 날씨·위치 기반으로 추천 → 클릭/좋아요 데이터 쌓임 → 나와 비슷한 사람 데이터와 합산해 통계/추천” 흐름은 추천 시스템에서 아주 전형적인 발전 과정

비지도 학습 + 지도학습이 섞인 추천 시스템 방식
1. 초기 추천
    - 날씨 데이터 + 위치 데이터 + (가능하다면) 기본 인기 맛집 데이터를 활용
    - 초기에는 고객 개인 데이터가 없으니 **일반 추천** 또는 **날씨·위치 기반 필터링**으로 시작
        
2. 사용자 행동 데이터 수집
    - 음식점 클릭, 좋아요, 예약, 후기 작성 등 행동 기록
    - 이 데이터는 정답 라벨 역할을 함 (이 고객은 어떤 상황에서 어떤 음식점을 선호했는가)
        
3. 개인화 학습
    - 나의 행동 패턴 + 나와 비슷한 취향을 가진 다른 사람들의 행동 패턴을 함께 사용
    - 예: 협업 필터링(Collaborative Filtering) + 콘텐츠 기반 추천(Content-based Filtering)
        
4. 추천 알고리즘 개선
    - 나의 과거 행동 데이터 + 그날 날씨 + 현재 위치 → 추천 확률 계산
    - 같은 조건에서 다른 사람들이 좋아요/클릭한 음식점 데이터도 반영 (통계 + 머신러닝)
---
핵심 포인트
- 날씨/위치는 초기 추천과 컨텍스트 기반 추천(Context-aware Recommendation)에 유용
- 클릭/좋아요 데이터는 개인 취향을 파악하는 데 핵심
- 다른 사람의 데이터를 함께 쓰면 콜드스타트 문제(초기 데이터 부족) 해결 가능
- 시간이 지날수록 추천 모델은 점점 개인화 + 정밀화됨

---
이미지 전처리 코드 (이미지 전처리(Image Preprocessing) 단계)
- 이미지 불러오기
    - 원본 데이터를 메모리에 읽어오는 단계
        
- 그레이스케일 변환
    - 색상 정보(RGB)를 제거하고 밝기(명암)만 남겨서 연산량을 줄임
    - 불필요한 색상 채널을 제거 → 속도 향상, 노이즈 감소
        
- 엣지 검출 (Canny)
    - 이미지에서 경계·윤곽선만 추출하여 특징을 단순화
    - 물체 형태나 구조 파악에 유용
        
- 결과 시각화
    - 전처리 결과를 눈으로 확인하고 파라미터 조정 가능


```python
import cv2  # OpenCV 라이브러리
import matplotlib.pyplot as plt

# ------------------------------------------------------------
# 1. 이미지 불러오기
# ------------------------------------------------------------
# 'sample.jpg'는 같은 폴더에 있는 이미지를 사용
# cv2.imread()로 이미지를 불러올 때 기본은 BGR 색상
image = cv2.imread('sample.jpg')

# 이미지를 정상적으로 불러왔는지 확인
if image is None:
    raise FileNotFoundError("이미지를 찾을 수 없습니다. 경로를 확인하세요.")

# ------------------------------------------------------------
# 2. 그레이스케일 변환
# ------------------------------------------------------------
# 컬러 이미지를 흑백(그레이스케일)으로 변환
gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

# ------------------------------------------------------------
# 3. 엣지(윤곽선) 검출
# ------------------------------------------------------------
# Canny 엣지 검출기 사용
edges = cv2.Canny(gray, threshold1=100, threshold2=200)

# ------------------------------------------------------------
# 4. 결과 출력 (matplotlib 사용)
# ------------------------------------------------------------
plt.figure(figsize=(12, 4))

# 원본 이미지 (BGR → RGB로 변환해서 출력)
plt.subplot(1, 3, 1)
plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))
plt.title('Original')
plt.axis('off')

# 그레이스케일 이미지
plt.subplot(1, 3, 2)
plt.imshow(gray, cmap='gray')
plt.title('Grayscale')
plt.axis('off')

# 엣지 이미지
plt.subplot(1, 3, 3)
plt.imshow(edges, cmap='gray')
plt.title('Edges')
plt.axis('off')

plt.show()
```

전처리 후 활용 예시
- 객체 검출(Object Detection) 전에 경계선을 추출하여 ROI(관심 영역)만 사용
- 이미지 분할(Segmentation) 전, 경계 정보를 얻어 정확도 향상
- OCR(문자 인식) 전에 배경 제거
- 얼굴 인식 전에 얼굴 경계만 추출
- 제조 검사에서 스크래치·불량 패턴만 검출

OpenCV 같은 경량 전처리 도구를 사용하여 전처리를 하고 성능 좋은 딥러닝/머신러닝 프레임워크로 작업하는것이 전형적인 작업 흐름입니다.

왜 이렇게 하냐?
1. 데이터 품질 개선
    - OpenCV는 이미지/영상의 크기 조정, 색상 보정, 노이즈 제거 등 빠르고 가벼운 전처리에 특화
    - 품질이 좋아진 데이터를 모델에 넣으면 인식률과 학습 효율이 올라감
        
2. 모델 부담 감소
    - 원본 그대로 모델에 넣으면 모델이 조명, 노이즈, 배경 변화를 다 처리해야 해서 성능 저하 가능
    - 전처리로 불필요한 변수를 줄이면 모델이 핵심 특징에 집중
        
3. 처리 속도 향상
    - OpenCV는 C++ 기반이라 연산이 빠름
    - 모델에 불필요한 큰 이미지를 넣는 대신, 전처리에서 최적 크기로 줄여서 속도 개선
---
전형적인 파이프라인 예시
1. OpenCV (전처리)
    - 이미지 로드, 리사이즈, 크롭, 색상 변환(BGR→RGB), 히스토그램 평활화, 노이즈 제거
    - 데이터 증강(회전, 반전, 밝기 변경)
2. 딥러닝 프레임워크 (주요 처리)
    - TensorFlow / PyTorch 기반 모델
    - YOLO, Faster R-CNN, Mask R-CNN 등 객체 감지
    - UNet, SegFormer 등 이미지 분할
    - ResNet, EfficientNet 등 분류(Classification)
        
3. OpenCV (후처리)
    - 모델 출력 후 바운딩 박스 그리기, 라벨 표시
    - 결과 저장, 동영상 합성, 웹 전송
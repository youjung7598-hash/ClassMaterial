#### 1) 최소 개념 5가지 

1. **라벨(Label)**: `http_requests_total{method="GET",status="200"}`
2. **메트릭 타입**: Counter(누적↑), Gauge(현재값), Histogram(분포→p95/p99)
3. **Pull 구조**: `target:port/metrics`를 **Prometheus**가 주기적으로 스크랩
4. **PromQL**: `rate()`, `sum by()`, `histogram_quantile()`만 알아도 실무 OK
5. **시계열**: 시간축으로 값이 변함(대시보드에서 추세로 문제 감지)
    
---
####  2) 준비물 & 포트

- FastAPI(8000), Django(8900), node_exporter(9100), Prometheus(9090), Grafana(3000), Alertmanager(9093)
- WSL/도커 혼용 시 Prometheus 컨테이너→호스트 서비스 접근:
    - 주소는 보통 `host.docker.internal:<port>`
    - Django는 `ALLOWED_HOSTS`에 `host.docker.internal` 포함 필요
        
---
Prometheus Targets
- 브라우저: `http://localhost:9090/targets`
- fastapi/django/node_exporter 상태가 **UP**이면 OK
---
### 핵심 PromQL 쿼리 (FastAPI / Django 분리)

트래픽 발생
```bash
# FastAPI
for i in {1..300}; do curl -s http://localhost:8000/ping >/dev/null; done

# Django
for i in {1..200}; do curl -s http://localhost:8900/ >/dev/null; done
```

FastAPI (prometheus-fastapi-instrumentator)
전체 RPS
```
sum(rate(http_requests_total{job="fastapi", handler!="/metrics"}[5m]))
```

메서드/상태별 RPS
```
sum by (method, status) (
  rate(http_requests_total{job="fastapi", handler!="/metrics"}[5m])
)
```

엔드포인트별 RPS
```
sum by (handler, method, status) (
  rate(http_requests_total{job="fastapi", handler!="/metrics"}[5m])
)
```

p95 응답지연
```
histogram_quantile(
  0.95,
  sum by (le) (
    rate(http_request_duration_seconds_bucket{job="fastapi"}[5m])
  )
)
```
---
Django (django-prometheus)
전체 RPS
```
sum(
  rate(django_http_responses_total_by_status_view_method_total{
    job="django", view!="prometheus-django-metrics"
  }[5m])
)
```

메서드/상태별 RPS
```
sum by (method, status) (
  rate(django_http_responses_total_by_status_view_method_total{
    job="django", view!="prometheus-django-metrics"
  }[5m])
)
```

뷰별 p95 응답지연
```
histogram_quantile(
  0.95,
  sum by (le) (
    rate(django_http_requests_latency_seconds_by_view_method_bucket{
      job="django"
    }[5m])
  )
)
```
---
Grafana 대시보드 (추천 6개 패널)

데이터소스: Prometheus (`http://localhost:9090`)  
시간 범위: Last 6 hours (처음엔 넉넉히)

Requests RPS (전체)
```
sum by (job) (
  rate(http_requests_total{handler!="/metrics"}[5m])
)
+ sum(
  rate(django_http_responses_total_by_status_view_method_total{
    view!="prometheus-django-metrics"
  }[5m])
)
```
두 메트릭 체계가 달라서 fastapi/django를 각각 구해 **패널 내에서 2개의 쿼리**로 추가하는 방식 권장.

FastAPI p95 Latency
```
histogram_quantile(0.95,
  sum by (le) (rate(http_request_duration_seconds_bucket{job="fastapi"}[5m]))
)
```

Django p95 Latency
```
histogram_quantile(0.95,
  sum by (le) (rate(django_http_requests_latency_seconds_by_view_method_bucket{job="django"}[5m]))
)
```

5xx Error Rate (전체)
```
(
  sum(rate(http_requests_total{status=~"5.."}[5m]))
  +
  sum(rate(django_http_responses_total_by_status_view_method_total{status=~"5.."}[5m]))
)
/
(
  sum(rate(http_requests_total[5m]))
  +
  sum(rate(django_http_responses_total_by_status_view_method_total[5m]))
)
```

In-Progress(동시 처리) — FastAPI만 사용 중이면  
(Instrumentator에서 `app_inprogress_requests`를 켠 경우에 한함. 없다면 생략)
```
app_inprogress_requests
```

시스템 자원 (Node Exporter)
CPU%
```
100 - avg by (instance) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100
```
Mem Available
```
node_memory_MemAvailable_bytes
```

트래픽/에러 시연 (그래프 반응 확인)
올바른 부하 명령 (이전 에러 원인: URL 누락 + xargs 옵션 충돌)

FastAPI에 400건/병렬 16
```
seq 400 | xargs -P16 -I{} curl -s -o /dev/null http://localhost:8000/
```

Django에 400건/병렬 16
```
seq 400 | xargs -P16 -I{} curl -s -o /dev/null http://localhost:8900/
```
또는 확실한 병렬:
```
for i in {1..400}; do curl -s -o /dev/null http://localhost:8000/ & done; wait
```

500 에러 유도(예시) FastAPI
```python
# src/routers/debug_router.py
from fastapi import APIRouter, HTTPException
router = APIRouter()
@router.get("/boom")
def boom(): raise HTTPException(status_code=500, detail="intentional")
```

```python
# src/app.py
from src.routers.debug_router import router as debug_router
app.include_router(debug_router)
```

요청:
```bash
for i in {1..30}; do curl -s -o /dev/null -w "%{http_code}\n" http://localhost:8000/boom; done
```

Django
```python
# restaurant/views.py
from django.http import HttpResponseServerError
def boom(request): return HttpResponseServerError("intentional")
```

```python
# restaurant/urls.py
from .views import boom
urlpatterns += [ path("boom/", boom) ]
```

요청:
```bash
for i in {1..30}; do curl -s -o /dev/null -w "%{http_code}\n" http://localhost:8900/boom/; done
```
Grafana에서 5xx 에러율과 p95 상승 확인!

---
알람 구성
Grafana Alert
1. Contact point: Webhook
    URL: `http://host.docker.internal:8800/grafana`  
    (Relay가 8800에서 실행 중이고 `.env`에 `DISCORD_WEBHOOK` 설정되어 있어야 함)
![[Pasted image 20250819184227.png]]

2. Notification policy: 기본 정책에 위 Contact point 연결
![[Pasted image 20250819184445.png]]

---
 패널 → Alerts 탭에서 규칙 추가:
 
 `1.` High 5xx Error Rate (FastAPI + Django 합산)
 
 Name  
`High 5xx Error Rate`

**Query (A)** — 그대로 붙여넣기
```
(
  sum(rate(http_requests_total{job="fastapi", handler!="/metrics", status=~"5.."}[5m])) +
  sum(rate(django_http_responses_total_by_status_view_method_total{job="django", view!="prometheus-django-metrics", status=~"5.."}[5m]))
)
/
(
  sum(rate(http_requests_total{job="fastapi", handler!="/metrics"}[5m])) +
  sum(rate(django_http_responses_total_by_status_view_method_total{job="django", view!="prometheus-django-metrics"}[5m]))
)
```

**Alert condition**
- `WHEN query A IS ABOVE 0.01`
- `FOR 5m` (5분 동안 기준 초과 유지 시 발화)

![[Pasted image 20250819182205.png]]

---
**Alert condition**
- `WHEN query A IS ABOVE 0.01`
- `FOR 5m` (5분 동안 기준 초과 유지 시 발화)
    
**Add folder and labels**
- Folder: (원하는 폴더 선택/신규 생성, 예: `Observability`)
- Labels: `severity=warning`, `service=web` (원하시는 라우팅 라벨 추가)
    
**Set evaluation behavior**
- Evaluate every: `30s`
- No data: `NoData` (기본)
- Error: `Alerting` (기본)
    
**Save rule**
> 설명: 두 서비스의 5xx 비율을 **합산/전체로 나눠** 에러율을 계산합니다. `/metrics`와 `prometheus-django-metrics`는 제외해서 노이즈를 줄였습니다.

![[Pasted image 20250819182227.png]]

---
- **Summary**: 한 줄 결론(무엇이 얼마를 넘었다)
- **Description**: 상세(임계치/최근값/다음 액션)
- **Runbook URL**: 대응 문서 링크(노션/깃헙 위키 등)

규칙별 예시
Summary
```
5xx error rate is high: {{ $values.A }} (> 0.01)
```

Description
```
Job: {{ $labels.job }}
Instance: {{ $labels.instance }}

최근 5분 평균 5xx 비율이 1%를 초과했습니다.
현재값(A) = {{ $values.A }} / 임계치 = 0.01

확인:
1) 최근 배포/트래픽 급증 여부
2) 에러 로그(스택/DB/외부 API)
3) 특정 핸들러에서만 치우치는지
```

![[Pasted image 20250819182245.png]]

---
Name  
`FastAPI p95 > 1.5s`

Query (A) 
```
histogram_quantile(
  0.95,
  sum by (le) (
    rate(http_request_duration_seconds_bucket{job="fastapi", handler!="/metrics"}[5m])
  )
)
```

Alert condition
- `WHEN query A IS ABOVE 1.5`
- `FOR 5m`

![[Pasted image 20250819182737.png]]

---
Add folder and labels
- Folder: (위와 동일)
- Labels: `severity=warning`, `service=fastapi`

Set evaluation behavior
- Evaluate every: `30s`
- No data: `NoData`
- Error: `Alerting`

Save rule
	설명: FastAPI의 응답시간 히스토그램으로 p95를 계산합니다(초 단위). `/metrics`는 제외했습니다.

![[Pasted image 20250819182804.png]]

---
Summary
```
FastAPI p95 latency high: {{ $values.A }}s (> 1.5s)
```

Description
```
Job: {{ $labels.job }}
Instance: {{ $labels.instance }}

최근 5분 p95 지연시간이 1.5초를 초과했습니다.
현재 p95(A) = {{ $values.A }}s / 임계치 = 1.5s

확인:
1) 특정 엔드포인트 지연? (라벨 핸들러별 패널 확인)
2) DB/외부 API 지연
3) CPU/메모리 스파이크 동반 여부
```

Runbook URL
팀 운영 문서 URL을 넣어 두세요. (예: `https://your-notion-or-wiki/runbooks/slow-requests`)
![[Pasted image 20250819182822.png]]

---
✅ 모니터링 고려 요소 (체크리스트)

1. **서비스 성격**
    - 웹/모바일 API 위주인지?
    - 실시간(WebRTC, 채팅, 게임 등)인지?
    - AI/추천/모델 서빙 기반인지?
    - 배치/스트리밍(ETL, Kafka, Spark 등)인지?
        
2. **사용자 경험(UX)에 중요한 지표**
    - 응답 성공률(5xx, 실패율)
    - 응답 지연(p95, p99)
    - 가용성(서비스 죽음/살아있음)
    - 품질 지표 (AI 정확도, 영상 지연/패킷손실률, 데이터 처리 지연 등)
        
3. **시스템 구조 & 의존성**
    - DB, 캐시(Redis), 메시지큐, 외부 API(결제, 인증) 등
    - 어디가 병목·단일 실패지점(SPOF)인지
        
4. **트래픽 패턴**
    - 요청이 폭주할 때? (성수기/이벤트)
    - 사용자가 적어도 오류율이 치명적인 경우?
        
5. **운영 환경**
    - 클라우드 vs 온프레미스 vs 로컬
    - 컨테이너(Kubernetes, Docker)인지 VM인지
        
6. **실패 모드**
    - 죽음(다운타임)
    - 느려짐(지연 상승)
    - 잘못된 응답(에러/품질 저하)
    - 비용 폭증(AI 토큰, 클라우드 리소스)
        
7. **보안·프라이버시**
    - 로그에 민감 정보가 섞이는지?
    - 외부로 내보내는 메트릭에 개인정보가 없는지?
        
8. **운영 팀의 목표 & 알람 대응 가능성**
    - 24/7 상주 팀이 있는지?
    - 즉시 대응 가능한 인력이 없으면 과한 알람은 오히려 해로움
        
9. **비용/효율성**
    - 수집 메트릭 개수·라벨이 과도하지 않은가? (Prometheus 비용 ↑)
    - 모니터링 툴 운영 비용이 감당 가능한가?
        
10. **확장성**
- 지금은 소규모지만, 나중에 AI·실시간·빅데이터 기능이 추가되었을 때 확장할 수 있는 구조인가?
    
---
👉 요약하면, “내 서비스가 언제, 무엇 때문에 실패하면 사용자에게 제일 치명적인가?” → 이것을 기준으로 모니터링 대상을 고르면 됩니다.


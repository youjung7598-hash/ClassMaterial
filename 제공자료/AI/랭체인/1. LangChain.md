OpenAI와 LangChain

- 장점
    - API 키만 있으면 바로 사용 가능 → 설치·훈련 과정 없이 “바로 실습” 가능.
    - 문서와 예제가 많고, LangChain 기본 튜토리얼도 대부분 OpenAI 기준.
    - 초보자도 빠르게 RAG(검색+질의응답), 챗봇, 요약 등 기능 구현 가능.
        
- 단점
    - API 유료 과금 구조 (토큰 단위).
    - 모델 내부 동작을 제어하거나 커스터마이즈하기 어렵다.
        
---

Hugging Face와 LangChain

- 장점
    - 무료 모델 다양 (Transformers, Hub에 수천 개 공개).
    - 로컬에서 직접 실행 가능 → API 비용 부담 없음.
    - 원하는 모델을 fine-tuning해서 서비스에 맞게 커스터마이징 가능.
        
- 단점
    - 환경 세팅이 복잡 (GPU 필요, 모델 크기 큼).
    - 속도/성능 이슈 → 최적화가 필요.
    - 초보자가 바로 연결하기엔 러닝커브가 있음.

처음엔 OpenAI로 쉽게 시작 → 나중에 Hugging Face로 확장

---
RAG란? 단순히 모델이 가진 지식만 쓰는 게 아니라, 외부 문서/데이터베이스에서 검색된 정보를 모델에게 붙여주고 답변을 생성하게 만드는 방식

- R = Retrieval (검색)
- A = Augmented (보강된)
- G = Generation (생성)

RAG는 GPT가 알지 못하는 최신/개인 데이터를 검색해와 답변에 반영하는 기술이고, LangChain은 이 과정을 쉽게 구현하도록 돕는 도구입니다.

**RAG의 원리:**
1. 질문이 들어오면 → 미리 저장해 둔 외부 문서/DB에서 관련 부분을 검색(Retrieval)
2. 검색된 핵심 정보 + 원래 질문을 AI에게 전달 
3. AI가 참고 자료를 바탕으로 생성(Generation) → 정확한 답변 제공

RAG를 요약하면:
- 내가 필요한 자료를 DB에 넣어두고,
- 질문하면 RAG가 그 DB에서 관련 내용을 찾아,
- GPT가 그걸 참고해 답을 생성하는 것
- 즉, 내 자료 + GPT가 결합된 시스템이 바로 RAG입니다.
        
**장점:**
- 오픈북 시험처럼 필요한 정보만 찾아서 활용
- 내부 문서, 매뉴얼, FAQ, PDF, Q&A 데이터 등에 적용 가능
- 고객센터·제품 가이드·행사 안내·온라인 쇼핑 Q&A 등 다양한 분야에서 실용적
        
**LangChain의 역할:**
- 데이터를 불러오고 쪼개고 검색하고 AI와 연결하는 과정을 레고 블록처럼 모듈화
- 개발자가 쉽게 RAG 기반 챗봇/서비스를 만들 수 있도록 지원

---
#### 랭체인으로 무엇을 어디까지 할수 있나?

랭체인이 잘하는 일 (AI 오케스트레이션)
- 프롬프트 체이닝: 질문 → 검색 → 요약 → 답변 같은 다단계 파이프라인
- RAG: 벡터DB(Chroma/FAISS/PGVector 등)로 사내 문서 검색 후 답변

==`잠깐 용어 설명:`==
	`벡터 DB(Vector Database)란?`
		`- 우리가 흔히 쓰는 DB(MySQL, PostgreSQL 등)는 데이터를 행/열 구조로 저장합니다.`
		`- 그런데 RAG에서는 텍스트(문서)를 숫자 벡터(예: 1536차원 배열)로 바꿔 저장해요.`
		`- 이렇게 하면 이 질문과 가장 비슷한 문서가 뭐지? 같은 유사도 검색이 가능해요. ` 
		    `→ 바로 이런 벡터 검색을 잘 해주는 DB가 Vector DB입니다.`

	대표적인 Vector DB 종류
	1. Chroma (크로마)
	    - Python 친화적인 오픈소스 벡터 DB
	    - LangChain 튜토리얼에서 가장 많이 등장
	    - 설치도 쉽고 로컬에서 빠르게 테스트 가능
        
	2. FAISS (파이시)
	    - Facebook AI가 만든 C++/Python 라이브러리
	    - Vector 검색만 딱 전문적으로 빠르게 해줌
	    - 초경량·고속이라 연구/프로토타입에 많이 쓰임
	    - DB라기보단 벡터 검색 엔진 느낌
        
	3. PGVector (Postgres + Vector 확장)
	    - PostgreSQL에 확장 기능을 달아서 벡터 검색까지 가능하게 한 것
	    - 즉, 기존 RDB(Postgres)에 AI용 벡터 컬럼을 추가한 버전
	    - 기업 환경에서 Postgres만 쓰고 싶은데 AI 검색도 하고 싶다 할 
	      때 자주 씀

RAG 동작 흐름(코딩관점)
FastAPI 기준 디렉토리 구조
```
rag_fastapi/
├─ app/
│  ├─ main.py                 # FastAPI 엔트리포인트
│  ├─ api/
│  │   └─ rag_routes.py       # RAG 관련 API 라우터
│  ├─ core/
│  │   └─ config.py           # 설정(예: API Key, DB 경로 등)
│  ├─ services/
│  │   ├─ rag_service.py      # RAG 처리 로직 (DB 검색 + LLM 호출)
│  │   └─ vector_store.py     # 벡터DB 초기화/저장/검색
│  └─ requirements.txt        # 의존성 (fastapi, langchain, chromadb 등)
└─ .env                       # 환경변수 (OPENAI_API_KEY 등)
```
- `vector_store.py` → Chroma/FAISS/PGVector 세팅
- `rag_service.py` → 질문 → 벡터 검색 → GPT 호출 → 답변 반환
- `rag_routes.py` → `/rag/query` 같은 엔드포인트 작성
- `main.py` → FastAPI 실행, 라우터 연결

Django + DRF 기준 디렉토리 구조
```
rag_django/
├─ manage.py
├─ config/
│  ├─ settings.py
│  ├─ urls.py
│  └─ asgi.py
├─ ragapp/                     # RAG 전용 앱
│  ├─ views.py                 # DRF APIView 또는 ViewSet
│  ├─ urls.py                  # /api/rag/ 경로 연결
│  ├─ services/
│  │   ├─ rag_service.py       # RAG 로직
│  │   └─ vector_store.py      # 벡터DB 관리
│  ├─ serializers.py           # (선택) 요청/응답 스키마
│  └─ models.py                # (선택) DB에 질의/로그 저장할 경우
└─ requirements.txt
```
- `ragapp/views.py` → DRF `APIView`로 `/api/rag/query/` 엔드포인트 제공
- `rag_service.py` → 벡터 검색 + GPT 호출
- `vector_store.py` → Chroma/PGVector 초기화

---
1️⃣데이터 준비
- 예: 책 내용, 회사 매뉴얼, 고객 Q&A등 정확한 답변을 요하는 문자데이터들
- 이걸 벡터(숫자 배열)로 변환해서 Vector DB(Chroma/FAISS/PGVector 등)에 저장
`services/vector_store.py`
```python
from sqlalchemy import create_engine
import pandas as pd
from langchain_openai import OpenAIEmbeddings
from langchain_chroma import Chroma

# 1) DB 연결 (예: SQLite → 다른 DB는 URL만 바꾸면 됨)
engine = create_engine("sqlite:///mydata.db")

# 2) 테이블에서 데이터 불러오기 (예: documents 테이블의 content 컬럼)
df = pd.read_sql("SELECT content FROM documents", engine)
texts = df["content"].tolist()

# 3) 임베딩 + Chroma 벡터DB 저장
embeddings = OpenAIEmbeddings()
vectordb = Chroma.from_texts(texts, embedding=embeddings)

def get_vectordb() -> Chroma:
    """다른 모듈에서 벡터DB 핸들만 가져다 쓰기 위함"""
    return _vectordb
```

2️⃣질문 입력 (검색창 연동) (질문 입력→검색→LLM 답변 생성
- 사용자의 질문(검색창/API 입력)을 받습니다.
- `vector_store.py`에서 준비한 벡터DB로 관련 문서 k개를 검색합니다.
- 검색 결과 + 질문을 GPT(LLM) 에 함께 전달하여, 맥락에 맞는 최종 답변을 생성합니다.  
    → 즉, 검색만이 아니라 검색+생성까지 한 방에 처리합니다.
`services/rag_service.py`
```python
from langchain_openai import ChatOpenAI
from langchain.chains import RetrievalQA
from .vector_store import get_vectordb

_llm = ChatOpenAI()  

def run_rag(user_question: str) -> str:
    """
    검색창/API로 들어온 질문을 받아
    → 벡터DB에서 관련 문서 k개 검색
    → LLM에 참고자료로 붙여 최종 답변 생성
    """
    retriever = get_vectordb().as_retriever(search_kwargs={"k": 2})
    qa = RetrievalQA.from_chain_type(llm=_llm, retriever=retriever)
    answer = qa.run(user_question)
    return answer
```

---
랭체인(LangChain)으로 만들 수 있는 것들
`1.` 챗봇 (Q&A)
- 예시: 고객센터 챗봇
    - 사용자가 “환불은 어떻게 하나요?”라고 묻는다 →
    - LangChain이 사내 문서(RAG)에서 관련 내용을 검색 →
    - OpenAI를 통해 답변 문장을 생성 →
    - 사용자에게 자연스러운 대화 형식으로 안내.
        
효과: 단순 FAQ 수준이 아니라, 내부 매뉴얼이나 PDF, DB 내용을 검색해 맞춤형 답변.

---
`2.` 요약기
- 예시: 회의록 요약기
    - 긴 회의 기록(10페이지)을 넣으면 →
    - LangChain 체인이 요약 단계별로 분리(핵심안건, 담당자, 액션아이템 등) →
    - 깔끔한 요약본 생성.
        
효과: 신입/팀원이 빠르게 맥락을 이해하도록 도와줌.

---
`3.` 데이터 분석 도우미
- 예시: 이 CSV에서 매출이 가장 높은 상품 알려줘
    - LangChain이 Pandas 같은 파이썬 툴을 불러 → CSV를 열고 → 연산 수행 → 답변.
        
효과: LLM이 코드 실행 도구와 함께 데이터 분석.

---
`4.` API 오케스트레이션
- 예시: 여행 예약 도우미
    - 사용자: 다음 주 토요일에 서울→부산 KTX 예약해줘
    - LangChain Agent가 `날짜 확인 → 코레일 API 호출 → 좌석 선택` 단계를 순서대로 실행.
        
효과: LLM이 “API 도구 선택”을 자동화.

---
`5.` 문서 생성/자동화
- 예시: 보고서 생성기
    - LangChain이 “DB 조회 → AI 분석 → 보고서 마크다운 작성 → PDF 변환”을 파이프라인으로 실행.
        
효과: 매번 사람이 하던 반복 보고서 업무 자동화.

---
✅ 한마디 정리  
LangChain은 LLM + 도구(검색, DB, API, 코드 실행 등)를 연결해서
- 챗봇
- 요약기
- 데이터 분석 도우미
- 자동화 에이전트
- 맞춤형 보고서 생성
    
같은 실무형 AI 서비스를 쉽게 만들 수 있는 오케스트레이션 프레임워크예요.
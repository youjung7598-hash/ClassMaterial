- Python 3.11 / LangChain 0.1.10 / GPT-3.5-turbo-0125
- RAG 최소골격: DB의 텍스트 → 임베딩 → Chroma(Vector DB) → 질문 입력 → 검색+생성

1️⃣ 준비하기
1. [OpenAI 홈페이지](https://platform.openai.com/) 회원가입
2. 로그인 → [API Keys](https://platform.openai.com/api-keys) 메뉴에서 새 키 발급
    - `sk-xxxxx...` 형태의 긴 문자열이 나와요. (이걸 비밀키처럼 잘 보관해야 함)
    
![[Pasted image 20250915133314.png]]

![[Pasted image 20250915133530.png]]
생성한 키는 한번밖에 확인이 안되므로 생성한 키를 잘 보관해둔다.

결제 세부정보 추가 버튼 클릭
![[Pasted image 20250915135928.png]]
→ 신용카드(또는 결제 수단)를 등록해야 API를 계속 쓸 수 있습니다.
- 등록 후에는 선호사항 → 사용 제한(Usage limit)에서
    - 월별 최대 결제 한도(예: $5~$10) 를 설정해두면
    - 과금 폭주를 방지할 수 있습니다.
        
- 결제 수단 등록이 끝나면 대시보드의 남은 크레딧이 업데이트되고,  
    이후 FastAPI에서 `/chat` 엔드포인트 호출이 정상 동작합니다.

2️⃣ 라이브러리 설치
터미널에서:
```bash
python -m venv .venv && source .venv/bin/activate
python -m pip install --upgrade pip
```
requirements.txt
```
fastapi
uvicorn[standard]
python-dotenv
pandas
sqlalchemy
langchain==0.1.10
langchain-openai
langchain-chroma
chromadb
```
설치:
```bash
pip install -r requirements.txt
```

3️⃣ 환경 변수에 키 저장
운영체제에 따라 다르지만, 터미널에서: 또는 .env에 저장
`.env`
```python
OPENAI_API_KEY=sk-발급받은키
OPENAI_MODEL=gpt-3.5-turbo-0125   # 원하면 gpt-4o-mini 등으로 교체
```

프로젝트 구조
```bash
your-project/
├─ app/
│  ├─ main.py                # FastAPI 엔트리포인트
│  ├─ api/
│  │   └─ rag_routes.py      # /rag/query 엔드포인트
│  └─ services/
│      ├─ vector_store.py    # 1) DB→임베딩→Chroma 초기화(준비)
│      └─ rag_service.py     # 2) 질문→검색→GPT 답변(실행)
├─ data/
│  └─ mydata.db              # SQLite 예시 (직접 생성/시드)
├─ requirements.txt
└─ .env
```

빠른 테스트 코드
DB 시드사용
`scripts/seed_sqlite.py`
```python
import sqlite3, os
os.makedirs("data", exist_ok=True)
conn = sqlite3.connect("data/mydata.db")
cur = conn.cursor()
cur.execute("CREATE TABLE IF NOT EXISTS documents (id INTEGER PRIMARY KEY, content TEXT)")
cur.execute("DELETE FROM documents")
cur.executemany(
    "INSERT INTO documents(content) VALUES(?)",
    [
        ("아빠 키 180cm"),
        ("가족 제주도 여행 출발 항공편은 N254 입니다."),
        ("가족 여행 일정표: 8월 15일 출발, 2박 3일"),
    ],
)
conn.commit()
conn.close()
print("OK: data/mydata.db seeded")
```
실행:
```bash
python scripts/seed_sqlite.py
```


4️⃣ RAG 핵심 코드1 (분리된 2파일)
`app/services/vector_store.py`
DB에서 텍스트를 읽고 → 임베딩(숫자 벡터)으로 변환 → Chroma에 담아 유사도 검색 준비를 끝냅니다.
```python
# app/services/vector_store.py
import os
from sqlalchemy import create_engine
import pandas as pd
from langchain_openai import OpenAIEmbeddings
from langchain_chroma import Chroma

DB_URL = os.getenv("DB_URL", "sqlite:///data/mydata.db")

# 1) DB에서 문서 로드
engine = create_engine(DB_URL)
df = pd.read_sql("SELECT content FROM documents", engine)
texts = df["content"].dropna().astype(str).tolist()

# 2) 임베딩 + Chroma 벡터DB 구성 (최소 예제: 영구화 생략)
_embeddings = OpenAIEmbeddings()
_vectordb = Chroma.from_texts(texts, embedding=_embeddings)

def get_vectordb() -> Chroma:
    """다른 모듈에서 벡터DB 핸들만 가져다 쓰기 위함"""
    return _vectordb
```

5️⃣ RAG 핵심2 코드
`app/services/vector_store.py`
질문을 받아 → 벡터 검색으로 관련 문서 k개를 찾고 → GPT(LLM) 에 “질문 + 검색결과”를 함께 넣어 최종 답변을 생성합니다.  
(= “검색만”이 아니라 “검색+생성”까지 한 번에 처리)
```python
# app/services/rag_service.py
import os
from langchain_openai import ChatOpenAI
from langchain.chains import RetrievalQA
from .vector_store import get_vectordb

MODEL = os.getenv("OPENAI_MODEL", "gpt-3.5-turbo-0125")
_llm = ChatOpenAI(model=MODEL)

def run_rag(user_question: str) -> dict:
    """
    (1) 사용자 질문 입력
    (2) 벡터DB에서 관련 문서 k개 검색
    (3) 검색 결과 + 질문을 GPT에 함께 전달하여 답변 생성
    """
    retriever = get_vectordb().as_retriever(search_kwargs={"k": 3})

    # 답변 생성
    qa = RetrievalQA.from_chain_type(llm=_llm, retriever=retriever)
    answer = qa.run(user_question)

    # (선택) 디버깅용: 실제 어떤 문서가 참조됐는지 보고 싶다면
    sources = [doc.page_content for doc in retriever.get_relevant_documents(user_question)]

    return {"answer": answer, "sources": sources}
```

6️⃣ FastAPI 라우터 & 실행 파일
`app/api/rag_routes.py`
```python
# app/api/rag_routes.py
from fastapi import APIRouter
from pydantic import BaseModel
from app.services.rag_service import run_rag

router = APIRouter(prefix="/rag", tags=["RAG"])

class Query(BaseModel):
    query: str

@router.post("/query")
def rag_query(payload: Query):
    return run_rag(payload.query)
```

7️⃣ `app/main.py`
```python
# app/main.py
import os
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from dotenv import load_dotenv
from app.api.rag_routes import router as rag_router

load_dotenv()  # .env 로드 (OPENAI_API_KEY, OPENAI_MODEL 등)

app = FastAPI(title="RAG Minimal API", version="1.0.0")
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"], allow_credentials=True,
    allow_methods=["*"], allow_headers=["*"],
)

app.include_router(rag_router)

@app.get("/")
def root():
    return {"ok": True, "msg": "RAG API ready"}
```

8️⃣ 실행
```bash
uvicorn app.main:app --reload --port 8000
```

6️⃣ 테스트:
```bash
curl -X POST http://localhost:8000/rag/query \
  -H "Content-Type: application/json" \
  -d '{"query":"우리 가족 제주도 출발 항공편은?"}'
```

성공 시:
```json
{ "answer": "제주도 출발 항공편은 N254입니다.", "sources": ["..."] }
```

---
LangChain·Chroma·OpenAI가 제대로 설치/동작하는지를 빠르게 확인 팁
코랩/주피터 빠른 확인
```python
!pip install langchain==0.1.10 langchain-openai langchain-chroma chromadb pandas sqlalchemy
import os, sqlite3, pandas as pd
os.environ["OPENAI_API_KEY"] = "sk-..."  # 임시 테스트용

# 1) 메모리 SQLite에 문서 넣기
conn = sqlite3.connect(":memory:")
cur = conn.cursor()
cur.execute("CREATE TABLE documents (content TEXT)")
cur.executemany("INSERT INTO documents(content) VALUES(?)", [
    ("아빠 키 180cm",),
    ("가족 제주도 여행 출발 항공편은 N254 입니다.",),
])
conn.commit()

# 2) Pandas로 불러와 Chroma 빌드
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_chroma import Chroma
from langchain.chains import RetrievalQA

df = pd.read_sql("SELECT content FROM documents", conn)
texts = df["content"].tolist()
emb = OpenAIEmbeddings()
vectordb = Chroma.from_texts(texts, embedding=emb)

# 3) 질문→검색→생성
llm = ChatOpenAI(model="gpt-3.5-turbo-0125")
qa = RetrievalQA.from_chain_type(llm=llm, retriever=vectordb.as_retriever({"k":2}))
print(qa.run("우리 가족 제주도 출발 항공편은?"))
```

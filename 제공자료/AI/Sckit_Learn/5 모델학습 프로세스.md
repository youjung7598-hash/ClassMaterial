ë¨¸ì‹ ëŸ¬ë‹ì˜ ê¸°ë³¸ íë¦„ 
	ì•„ëž˜ íë¦„ì€ scikit-learn ê¸°ë°˜ì˜ ì „í˜•ì ì¸ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ í•™ìŠµ ì ˆì°¨ì´ë©°, ëŒ€ë¶€ë¶„ì˜ ë¨¸ì‹ ëŸ¬ë‹ ë¬¸ì œì— ê¸°ë³¸ ë¼ˆëŒ€ì²˜ëŸ¼ ê³µí†µì ìœ¼ë¡œ ì ìš©ë©ë‹ˆë‹¤.
	
 `1.` ë°ì´í„° ì „ì²˜ë¦¬ (Preprocessing)
- ì„¤ëª…: ë°ì´í„°ì— ìžˆëŠ” ê²°ì¸¡ì¹˜, ì´ìƒì¹˜, í…ìŠ¤íŠ¸ ë“±ì„ ì •ë¦¬í•´ì„œ ë¨¸ì‹ ëŸ¬ë‹ì— ì í•©í•˜ê²Œ ë§Œë“œëŠ” ë‹¨ê³„
    - ì˜¤ë¥˜ ì œê±°: ìž˜ëª»ëœ ê°’, ì´ìƒì¹˜(outlier) ì œê±°
    - ëˆ„ë½ëœ ê°’ ì²˜ë¦¬: ë¹„ì–´ ìžˆëŠ” ë°ì´í„°ë¥¼ ì±„ìš°ê±°ë‚˜ ì œê±° (`fillna`, `dropna`)
    - ë²”ì£¼í˜• ë°ì´í„° ë³€í™˜: ë¬¸ìžë¥¼ ìˆ«ìžë¡œ ë°”ê¿”ì£¼ëŠ” ìž‘ì—… (`LabelEncoder`, `OneHotEncoder`)
    - ìŠ¤ì¼€ì¼ë§: ë°ì´í„°ì˜ ê°’ ë²”ìœ„ë¥¼ ì¼ì •í•˜ê²Œ ë§žì¶”ê¸° (`StandardScaler`, `MinMaxScaler`)
    - ë°ì´í„° ë¶„í• : í•™ìŠµìš©/ê²€ì¦ìš©/í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ë°ì´í„°ë¥¼ ë‚˜ëˆ„ê¸° (`train_test_split`)

ì›ì²œ ë°ì´í„° ì „ì²˜ë¦¬ ì˜ˆì‹œ ì½”ë“œ
ìš°ë¦¬ê°€ í”ížˆ ë‹¤ë£¨ëŠ” CSV íŒŒì¼ í˜•íƒœì˜ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì™€ì„œ, ê²°ì¸¡ì¹˜ ì²˜ë¦¬, ì´ìƒì¹˜ ì œê±°, ë²”ì£¼í˜• ì²˜ë¦¬, ìŠ¤ì¼€ì¼ë§, ë°ì´í„° ë¶„í• ê¹Œì§€ ì „ì²´ ê³¼ì •:
- CSV íŒŒì¼: `data.csv`
- ì»¬ëŸ¼:
    - `age`: ë‚˜ì´ (ìˆ«ìž)
    - `income`: ìˆ˜ìž… (ìˆ«ìž)
    - `gender`: ì„±ë³„ (ë‚¨ìž/ì—¬ìž)
    - `bought`: êµ¬ë§¤ ì—¬ë¶€ (0 or 1)

```python
import pandas as pd
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split

# 1. CSV íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°
df = pd.read_csv("data.csv")
print("[ì›ë³¸ ë°ì´í„° ìƒìœ„ 5ê°œ]\n", df.head())

# 2. ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ì¤€ë¹„
df.replace(["null", ""], pd.NA, inplace=True)  # ë¬¸ìžì—´ null ë˜ëŠ” ë¹ˆì¹¸ì„ NaNìœ¼ë¡œ
df["age"] = pd.to_numeric(df["age"], errors="coerce")
df["income"] = pd.to_numeric(df["income"], errors="coerce")

# ê²°ì¸¡ì¹˜ ê°œìˆ˜ ì¶œë ¥
print("\n[ê²°ì¸¡ì¹˜ ê°œìˆ˜ í™•ì¸]\n", df.isnull().sum())

# ê²°ì¸¡ì¹˜ ì±„ìš°ê¸°: í‰ê·  ë˜ëŠ” ìµœë¹ˆê°’
df["age"].fillna(df["age"].mean(), inplace=True)
df["income"].fillna(df["income"].mean(), inplace=True)
df["gender"].fillna(df["gender"].mode()[0], inplace=True)

print("\n[ê²°ì¸¡ì¹˜ ì²˜ë¦¬ í›„ ìƒìœ„ 5ê°œ]\n", df.head())

# 3. ì´ìƒì¹˜ ì œê±° (ì˜ˆ: income > 20000 ì œê±°)
before_count = len(df)
df = df[df["income"] < 20000]
after_count = len(df)
print(f"\n[ì´ìƒì¹˜ ì œê±°] {before_count - after_count}ê±´ ì œê±°ë¨ (income > 20000)\n")

# 4. ë²”ì£¼í˜• ì²˜ë¦¬: gender ì¸ì½”ë”© (ë‚¨ìž â†’ 1, ì—¬ìž â†’ 0 ë“±)
le = LabelEncoder()
df["gender"] = le.fit_transform(df["gender"])
print("\n [ì„±ë³„ ì¸ì½”ë”© ê²°ê³¼ (0=ì—¬ìž, 1=ë‚¨ìž)]\n", df[["gender"]].head())

# 5. ìž…ë ¥(X), ì¶œë ¥(y) ë¶„ë¦¬
X = df[["age", "income", "gender"]]
y = df["bought"]

print("\n[íƒ€ê²Ÿê°’ ë¶„í¬]\n", y.value_counts())

# 6. ìŠ¤ì¼€ì¼ë§ (StandardScaler: í‰ê· =0, í‘œì¤€íŽ¸ì°¨=1)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

print("\n[ìŠ¤ì¼€ì¼ë§ í›„ ìƒ˜í”Œ]\n", pd.DataFrame(X_scaled, columns=X.columns).head())

# 7. ë°ì´í„° ë¶„í•  (train: 80%, test: 20%)
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42
)

print("\n[ë°ì´í„° ë¶„í•  ê²°ê³¼]")
print("X_train shape:", X_train.shape) 
# í•™ìŠµìš© ìž…ë ¥ ë°ì´í„° (íŠ¹ì„±)ì˜ í–‰/ì—´ ê°œìˆ˜

print("X_test shape :", X_test.shape)    
# í…ŒìŠ¤íŠ¸ìš© ìž…ë ¥ ë°ì´í„° (íŠ¹ì„±)ì˜ í–‰/ì—´ ê°œìˆ˜

print("y_train shape:", y_train.shape)   
# í•™ìŠµìš© ì •ë‹µ(íƒ€ê²Ÿ) ë°ì´í„°ì˜ ê°œìˆ˜

print("y_test shape :", y_test.shape)    
# í…ŒìŠ¤íŠ¸ìš© ì •ë‹µ(íƒ€ê²Ÿ) ë°ì´í„°ì˜ ê°œìˆ˜
```
- `dropna()` : ê²°ì¸¡ì¹˜ê°€ ìžˆëŠ” í–‰ ì œê±°
- `LabelEncoder()` : ë¬¸ìží˜• â†’ ìˆ«ìží˜•ìœ¼ë¡œ ì¸ì½”ë”©
- `StandardScaler()` : í‰ê· =0, í‘œì¤€íŽ¸ì°¨=1ë¡œ ì •ê·œí™”
- `train_test_split()` : ë°ì´í„°ë¥¼ 80:20ìœ¼ë¡œ ë‚˜ëˆ„ì–´ í•™ìŠµ/í‰ê°€ìš©ìœ¼ë¡œ ì‚¬ìš©

ì˜ˆì‹œìš© CSV (`data.csv`)
```
age,income,gender,bought
25,3000,ë‚¨ìž,0
32,4500,ì—¬ìž,1
28,5000,ë‚¨ìž,1
35,7000,ì—¬ìž,0
40,,ì—¬ìž,1
19,2000,ë‚¨ìž,0
50,8000,ì—¬ìž,1
37,9000,ì—¬ìž,1
22,2500,ë‚¨ìž,0
41,6000,ì—¬ìž,1
38,1000000,ë‚¨ìž,0
26,3200,ì—¬ìž,0
31,4700,ì—¬ìž,1
29,5100,ë‚¨ìž,1
null,4000,ì—¬ìž,0
36,,ë‚¨ìž,1
34,6800,,1
45,7200,ì—¬ìž,1
52,8200,ë‚¨ìž,0
27,3100,ë‚¨ìž,1
```
ìœ„ ì˜ˆì‹œì—ì„œ ìˆ˜ìž…ì´ ì—†ëŠ” í–‰(`income` ë¹„ì–´ìžˆìŒ)ì€ ì œê±°ë©ë‹ˆë‹¤.

---
`2.` íŠ¹ì„± ì„ íƒ (Feature Selection)
- ì„¤ëª…: ëª¨ë“  ë°ì´í„° ì»¬ëŸ¼ì´ ëª¨ë¸ ì„±ëŠ¥ì— ë„ì›€ ë˜ëŠ” ê²ƒì€ ì•„ë‹™ë‹ˆë‹¤.  ë¶ˆí•„ìš”í•˜ê±°ë‚˜ ì¤‘ë³µëœ íŠ¹ì„±ì„ ì œê±°í•˜ë©´ ëª¨ë¸ì´ ë” ë¹ ë¥´ê²Œ í•™ìŠµí•˜ê³ , ì„±ëŠ¥ì´ ë” ì¢‹ì•„ì§ˆ ìˆ˜ ìžˆìŠµë‹ˆë‹¤.
    - ëª¨ë¸ì˜ ë³µìž¡ë„ë¥¼ ì¤„ì´ê³ 
	- í•™ìŠµ ì‹œê°„ì„ ë‹¨ì¶•í•˜ë©°
	- ê³¼ì í•©(overfitting) ìœ„í—˜ë„ ì¤„ìž„
íŠ¹ì„± ì„ íƒ ë°©ë²•
- íŠ¹ì„± ì¤‘ìš”ë„ í‰ê°€  
    ê³„ìˆ˜ ê¸°ë°˜ (`LinearRegression` ë“±), íŠ¸ë¦¬ ê¸°ë°˜ (`RandomForest`, `XGBoost` ë“±)
- ìƒê´€ê´€ê³„ ë¶„ì„  
    ì„œë¡œ ë„ˆë¬´ ë¹„ìŠ·í•œ íŠ¹ì„± ì œê±° (ì˜ˆ: ìƒê´€ê³„ìˆ˜ 0.9 ì´ìƒ)
- ì°¨ì› ì¶•ì†Œ ê¸°ë²•  
    PCA, t-SNE, LDA ë“±ìœ¼ë¡œ íŠ¹ì„± ì••ì¶•

ì˜ˆì œ ëª©ì 
- ë°ì´í„°ì— ì—¬ëŸ¬ íŠ¹ì„±ì´ ìžˆì„ ë•Œ, ëª¨ë¸ ì„±ëŠ¥ì— ë„ì›€ ì•ˆ ë˜ëŠ” ê²ƒë“¤ì„ ì œê±°
- ì´ë¥¼ í†µí•´ ëª¨ë¸ ì„±ëŠ¥ í–¥ìƒ, ê³¼ì í•© ë°©ì§€, í•™ìŠµ ì†ë„ í–¥ìƒ
ê°€ìƒì˜ ë°ì´í„° êµ¬ì¡°
```'
age,income,gender,zipcode,random_noise,bought
25,3000,ë‚¨ìž,12345,0.32,0
...
```
- `zipcode`, `random_noise` ê°™ì€ ì»¬ëŸ¼ì€ ì˜ˆì¸¡ì— ë³„ ë„ì›€ì´ ì•ˆ ë  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.
- ìš°ë¦¬ëŠ” ì´ëŸ° ë¶ˆí•„ìš”í•œ íŠ¹ì„±ì„ ìžë™ìœ¼ë¡œ ê±¸ëŸ¬ë‚¼ ìˆ˜ ìžˆì–´ìš”.

ì˜ˆì‹œ 1: íŠ¹ì„± ì¤‘ìš”ë„ ê¸°ë°˜ (íŠ¸ë¦¬ ëª¨ë¸ ì‚¬ìš©)
```python
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split

# 1. ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
df = pd.read_csv("data.csv")
df = df.dropna()

# 2. ë²”ì£¼í˜• â†’ ìˆ«ìží˜•
le = LabelEncoder()
df['gender'] = le.fit_transform(df['gender'])

# 3. ìž…ë ¥(X), ì¶œë ¥(y) ë¶„ë¦¬
X = df.drop("bought", axis=1)
y = df["bought"]

# 4. í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë¶„ë¦¬
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)

# 5. ëžœë¤ í¬ë ˆìŠ¤íŠ¸ ëª¨ë¸ë¡œ ì¤‘ìš”ë„ ê³„ì‚°
model = RandomForestClassifier()
model.fit(X_train, y_train)

# 6. íŠ¹ì„± ì¤‘ìš”ë„ ì¶œë ¥
importances = model.feature_importances_
feature_names = X.columns

for name, importance in zip(feature_names, importances):
    print(f"{name}: {importance:.4f}")
```

ì˜ˆì‹œ 2: ìƒê´€ê´€ê³„ ë¶„ì„ìœ¼ë¡œ ì¤‘ë³µ íŠ¹ì„± ì œê±°
```python
import seaborn as sns
import matplotlib.pyplot as plt

# ìƒê´€ê´€ê³„ í–‰ë ¬ ê³„ì‚°
corr_matrix = df.corr(numeric_only=True)

# ì‹œê°í™”
sns.heatmap(corr_matrix, annot=True, cmap="coolwarm")
plt.title("Feature Correlation Heatmap")
plt.show()

# ìƒê´€ê³„ìˆ˜ 0.9 ì´ìƒì¸ ì»¬ëŸ¼ë“¤ ì œê±°í•˜ê¸° (ì˜ˆ: ë„ˆë¬´ ìœ ì‚¬í•œ ê²ƒ)
upper_triangle = corr_matrix.where(
    (abs(corr_matrix) > 0.9) & (abs(corr_matrix) < 1.0)
).stack()

print("ì„œë¡œ ë„ˆë¬´ ìœ ì‚¬í•œ íŠ¹ì„±ë“¤:\n", upper_triangle)
```

ì˜ˆì‹œ 3: ì°¨ì› ì¶•ì†Œ (PCA)
```python
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

# ìŠ¤ì¼€ì¼ë§ ë¨¼ì € í•„ìš”
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# PCA ì ìš© (ì£¼ì„±ë¶„ 2ê°œë¡œ ì¤„ì—¬ë´„)
pca = PCA(n_components=2)
X_reduced = pca.fit_transform(X_scaled)

print("ì°¨ì› ì¶•ì†Œëœ ë°ì´í„°:\n", X_reduced[:5])
```

---
`3.` ëª¨ë¸ ì„ íƒ ë° í›ˆë ¨ (Model Training)
- ì„¤ëª…: í•™ìŠµí•  ì•Œê³ ë¦¬ì¦˜ì„ ì„ íƒí•˜ê³ , ë°ì´í„°ë¥¼ í•™ìŠµì‹œì¼œ ëª¨ë¸ì„ ë§Œë“œëŠ” ë‹¨ê³„ ëª¨ë¸ì„ íƒë„ ì¤‘ìš”í•˜ì§€ë§Œ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ê³  í”¼ì²˜ë¥¼ ë½‘ëŠ”ê²ƒì´ ê°€ìž¥ ì‹œê°„ì´ ì˜¤ëž˜ê±¸ë¦¬ê³  ì¤‘ìš”í•œ ì¼ìž…ë‹ˆë‹¤.
    - `LinearRegression()`, `RandomForestClassifier()`
    - `.fit(X_train, y_train)` ìœ¼ë¡œ í•™ìŠµ

`4.` ëª¨ë¸ í‰ê°€ ë° ìµœì í™” (Evaluation & Tuning)
- ì„¤ëª…: ëª¨ë¸ì´ ì–¼ë§ˆë‚˜ ìž˜ ì˜ˆì¸¡í•˜ëŠ”ì§€ í‰ê°€í•˜ê³ , ë” ì¢‹ì€ ê²°ê³¼ê°€ ë‚˜ì˜¤ê²Œ ê°œì„ í•˜ëŠ” ë‹¨ê³„ìž…ë‹ˆë‹¤.
    - `.predict(X_test)` â†’ ì˜ˆì¸¡ê°’
    - `accuracy_score`, `mean_squared_error` ë“±ìœ¼ë¡œ í‰ê°€
    - `GridSearchCV`ë¡œ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹

ðŸ”„ íë¦„ë„ ìš”ì•½ ê·¸ë¦¼
```
ë°ì´í„° â†’ ì „ì²˜ë¦¬ â†’ ì¤‘ìš”í•œíŠ¹ì„± ì„ íƒ â†’ ëª¨ë¸ í•™ìŠµ â†’ ëª¨ë¸ í‰ê°€ â†’ ë”ë‚˜ì€ ëª¨ë¸
```

ì‹¤ì œ ì½”ë“œ íë¦„ ì˜ˆì‹œ (scikit-learn)
```python
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# 1. ë°ì´í„° ì¤€ë¹„ & ì „ì²˜ë¦¬
X_train, X_test, y_train, y_test = train_test_split(X, y)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# 2. ëª¨ë¸ í›ˆë ¨
model = LogisticRegression()
model.fit(X_train, y_train)

# 3. í‰ê°€
y_pred = model.predict(X_test)
print("ì •í™•ë„:", accuracy_score(y_test, y_pred))
```

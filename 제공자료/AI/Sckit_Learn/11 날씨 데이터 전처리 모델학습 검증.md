
데이터 전처리 스케일링
데이터의 크기(범위)를 일정한 기준으로 맞춰주는 과정으로 쉽게 말해, 변수(컬럼)마다 단위나 범위가 다르면 모델이 특정 변수만 크게 반영하거나 작게 반영하는 문제가 생기는데, 이걸 방지하려고 크기를 조정하는 겁니다.

`1.` 왜 스케일링이 필요한가?
    - `온도`: 10~30
    - `강수량`: 0~100
    - `풍속`: 0~5  
        모델이 학습할 때 값의 절대 크기가 크면 그 변수를 더 중요하게 생각하는 경향이 있어요.  그래서 각 변수의 평균과 분산을 맞춰주면, 공정하게 비교할 수 있습니다.
        
`2.` StandardScaler의 원리
`StandardScaler`는 표준화(Standardization) 방법을 사용합니다.

- 변환 공식
  z = x−μ \ σ

- x: 원래 값
- μ: 평균 (mean)
- σ: 표준편차 (standard deviation)
- 변환 결과:
    - 평균(mean) = 0
    - 표준편차(std) = 1
    - 값은 -3 ~ 3 사이로 주로 분포

스케일링 (StandardScaler) → 숫자 범위 맞추기
```python
# 데이터 전처리 (스케일링)
from sklearn.preprocessing import StandardScaler
import pandas as pd
import numpy as np

# CSV 불러오기
data = pd.read_csv('data/weather_data_updated.csv')

# 종속변수(Category) 제외
x = data.drop("Category", axis=1)
print(x)  # 스케일링 전 데이터

# 스케일러 초기화 & 학습 후 변환
scaler = StandardScaler()
scaled_data = scaler.fit_transform(x)

# 스케일링된 데이터 출력
print(scaled_data)  # numpy array 형태 (평균=0, 표준편차=1)
```

###### 스케일링 전
| Temperature | Precipitation | Cloudiness | Snowfall | Pressure |
| ----------- | ------------- | ---------- | -------- | -------- |
| 29.29       | 0.00          | 23.14      | 0.0      | 1008.70  |
| 18.59       | 0.00          | 51.22      | 0.0      | 1018.49  |
| 25.84       | 94.47         | 17.05      | 0.0      | 997.11   |
| 12.11       | 0.00          | 36.57      | 0.0      | 1006.40  |
| 10.89       | 0.00          | 39.40      | 0.0      | 994.98   |
##### 스케일링 후
| Temperature_sc | Precipitation_sc | Cloudiness_sc | Snowfall_sc | Pressure_sc |
| -------------- | ---------------- | ------------- | ----------- | ----------- |
| **1.80**       | **-0.56**        | **-0.70**     | **-0.47**   | **0.17**    |
| **0.73**       | **-0.56**        | **0.29**      | **-0.47**   | **1.13**    |
| **1.45**       | **2.90**         | **-0.92**     | **-0.47**   | **-0.97**   |
| **0.08**       | **-0.56**        | **-0.23**     | **-0.47**   | **-0.06**   |
| **-0.04**      | **-0.56**        | **-0.13**     | **-0.47**   | **-1.18**   |

차이를 쉽게 이해하는 법
1. 범위 통일
    - 스케일링 전에는 온도(10~30), 강수량(0~94), 기압(990~1025)처럼 값의 범위가 제각각.
    - 스케일링 후에는 모든 컬럼이 ==평균 0, 표준편차 1 기준으로 변환== → 범위가 비슷해져서 모델이 공정하게 비교 가능.
    - 수치의 범위는 제한되지 않으며  ±3 정도까지 나옵니다.
		
`StandardScaler`는 데이터의 퍼짐 정도(분산)를 유지하면서 단위만 표준편차 기준으로 바꾸기 때문이에요.
- 1.45 → 평균보다 1.45 × 표준편차만큼 높음
- 2.0 이상 → 평균보다 매우 높은 값 (상위 약 2.5%에 해당)
- 0.0 → 평균값
- -1.0 → 평균보다 표준편차만큼 낮음
        
2. 값의 크기 해석
    - 양수 → 평균보다 큰 값
    - 음수 → 평균보다 작은 값
    - 절대값이 클수록 평균에서 멀리 떨어져 있음.
        
3. 예시 해석
    - 첫 번째 행: 온도(1.80) → 평균보다 많이 높음, 강수량(-0.56) → 평균보다 적음, 기압(0.17) → 평균보다 약간 높음.
    - 세 번째 행: 강수량(2.90) → 평균보다 매우 많음.
4. 계절 평균을 쓰는 경우
	- 날씨 예측처럼 계절성이 매우 강한 시계열 분석 → 계절별 평균을 쓰는 게 유리
	- 추천 시스템, 패턴 인식처럼 전체 기간을 아우르는 패턴 분석 → 전체 평균을 쓰는 게 일반적


데이터 전처리 (범주형 데이터) One-Hot Encoding(원-핫 인코딩) 방식
```python
from sklearn.preprocessing import OneHotEncoder
import pandas as pd

# 데이터
data = pd.read_csv('data/weather_data_updated.csv')

df = pd.DataFrame(data)

# OneHotEncoder 적용
encoder = OneHotEncoder()
encoded_data = encoder.fit_transform(df[['Category']]).toarray()

# 인코딩된 데이터를 DataFrame으로 변환
encoded_df = pd.DataFrame(encoded_data, columns=encoder.get_feature_names_out(['Category']))

print(encoded_df)
```

`1.` 범주형 데이터(Categorical Data)란?
- 값이 카테고리(분류명)로 되어 있는 데이터
- 예: `날씨` → `맑음`, `흐림`, `비`, `눈`
- 숫자가 아니라 의미를 가진 라벨이기 때문에, 바로 수학 연산에 쓰기 어렵습니다.

`2.` 왜 인코딩이 필요한가?
머신러닝 모델(특히 회귀, 신경망 등)은 숫자만 처리할 수 있어서,  
범주형 데이터를 숫자 형태로 변환해야 합니다.

`3.` One-Hot Encoding이란?
각 카테고리를 서로 다른 컬럼으로 만들고, 해당되는 컬럼에만 1을 넣고 나머지는 0을 넣는 방식입니다.

예시
`Category`:
```
분식 
디저트 
찜 
초밥 
치킨
```

One-Hot Encoding을 적용해서  
`Category`(음식 종류) 컬럼을 모델이 학습할 수 있는 숫자 벡터 형태로 변환한 것입니다.
![[Pasted image 20250806152139.png]]
의미
- 각 컬럼 → 특정 음식 종류를 나타냄 (`Category_국밥`, `Category_디저트`, `Category_리멘` …)
- 각 행(row) → 한 건의 데이터 (날짜+날씨+음식 카테고리)에서, 해당 음식에만 1이 들어가고 나머지는 0
- 즉, 행 하나에 1만 하나 있고 나머지는 전부 0 → 그 행이 어떤 음식 종류에 해당하는지 표시
---
왜 이렇게 변환하는가?
머신러닝 모델은 문자형 데이터(`분식`, `디저트`…)를 직접 이해 못하므로:
1. 각 음식 카테고리를 독립 변수(컬럼)로 만들고
2. 해당 행이 속한 음식에만 1을 넣어 범주 정보를 숫자로 표현함
    
이렇게 하면 날씨, 기온, 강수량 같은 수치형 데이터와  
카테고리(음식 종류)를 함께 모델 학습에 투입할 수 있습니다.

---
정리
- `encoded_df`의 각 열은 음식 종류 하나를 나타냄
- `1` → 해당 데이터가 그 음식 종류임
- `0` → 해당 데이터는 그 음식 종류가 아님
- 이진 플래그처럼 범주 정보를 숫자로 표현한 것

---
날씨에 따른 음식 추천 모델을 만들 때,  
어떤 날씨 요소(온도, 강수량, 기압 등)가 음식 선택에 영향을 많이 주는지를 정량적으로 확인

랜덤 포레스트란?
- 여러 개의 결정 트리(의사결정 나무)를 만들어 다수결로 분류하는 앙상블 모델
- 장점: 과적합에 강하고, 비선형 문제도 잘 처리함
- 보너스 기능: 학습이 끝난 뒤, 각 피처(컬럼)의 중요도를 자동 계산해줍니다

---
랜덤 포레스트(Random Forest) 모델을 학습해서 피처 중요도(feature importance)를 계산·시각화하는 코드
```python
# RF 피처 중요도 추출
from sklearn.ensemble import RandomForestClassifier
import pandas as pd

# 데이터
data = pd.read_csv('data/weather_data_updated.csv')

# 피처(X)와 타겟(y) 분리
X = data.drop("Category", axis=1)
y = data["Category"]

# 랜덤 포레스트 분류기 생성 및 훈련
rf_clf = RandomForestClassifier(n_estimators=100, random_state=1)
rf_clf.fit(X, y)

# 피처 중요도 추출 및 출력
feature_importances = rf_clf.feature_importances_

# 피처 이름 추출
feature_names = X.columns

# 피처 중요도와 함께 데이터프레임 생성
importances_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importances})

# 중요도에 따라 데이터프레임 정렬
importances_df = importances_df.sort_values(by='Importance', ascending=False)

# 출력
print(importances_df)

# 시각화
import matplotlib.pyplot as plt

plt.figure(figsize=(8,5))
plt.barh(importances_df['Feature'], importances_df['Importance'])
plt.xlabel("Importance")
plt.title("Feature Importance in RF")
plt.gca().invert_yaxis()
plt.show()
```

출력결과
```
         Feature  Importance
4       Pressure    0.392109
0    Temperature    0.272112
2     Cloudiness    0.214044
1  Precipitation    0.088088
3       Snowfall    0.033647
```
- 가장 중요한 변수: 기압 (Pressure) → `0.392`, 즉 전체 모델 결정에 39.2% 기여
- 그 다음은 온도(27.2%), 구름량(21.4%)
- 반면에 강수량(8.8%)과 눈(3.3%)은 상대적으로 기여도가 낮음

해석 요약:
- 이 데이터에선 기압이 가장 음식 선택과 관련성이 높고,
- 그 다음은 기온과 구름량이 영향을 미친다는 뜻입니다.
- 눈(Snowfall)은 영향이 매우 적으므로, 예측 모델에서 중요하지 않을 수 있음

이걸 왜 쓰냐?
- 어떤 피처가 가장 중요하고 영향력 있는지 판단해서:
    - 모델 해석력 향상
    - 불필요한 피처 제거 (속도 향상, 성능 향상)
    - 사용자에게 설명 가능한 추천 시스템 만들기 (Explainable AI)

피처(X)와 타겟(y)을 분리하는 이유
- 머신러닝은 “입력(X)”을 넣어서 “정답(y)”을 예측하는 구조입니다.
- 랜덤포레스트를 포함한 모든 지도학습(분류/회귀) 알고리즘에서  
    “입력(X)”과 “정답(y)”을 명확히 구분해서 학습합니다.
- 예를 들어,
	- X : 예측에 사용할 데이터 (날씨 정보 등)
	- y : 예측하고자 하는 값 (음식 종류 등)

만약 구분하지 않으면?
- 모델이 뭘 보고 뭘 예측해야 할지 몰라서,  
    학습 자체가 불가능해요!

![[Pasted image 20250806153137.png]]

입력 변수(피처)와 타겟(목표값)의 차이
- Feature(피처, 입력 변수)**:
    - `Temperature`, `Precipitation`, `Cloudiness`, `Snowfall`, `Pressure`
    - 이 5개가 각 행(샘플)에 대해 기록된 날씨 정보입니다.
- Target(타겟, 정답값):
    - `Category`
    - 이건 그날의 음식 카테고리(분식, 디저트, 초밥, 찜, 스테이크 등 10가지 중 하나)
---
날씨 데이터에서 결측치·이상치를 먼저 처리한 뒤, 랜덤 포레스트(Random Forest)로 각 피처(변수)가 분류 결과에 얼마나 중요한지 계산하는 코드
```python
# RF 피처 중요도 추출 (결측치 이상치 처리)
from sklearn.ensemble import RandomForestClassifier
import pandas as pd

# 데이터
data = pd.read_csv('data/weather_data_updated.csv')

# 결측치 확인 및 처리
data.isnull().sum()  # 결측치 확인
data.fillna(method='ffill', inplace=True)  
# 결측치 처리 (예: 앞의 값으로 채우기)

def remove_outliers(df, column):
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    # 이상치가 아닌 데이터만 필터링
    print(df[(df[column] < lower_bound) | (df[column] > upper_bound)])
    filtered_df = df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]
    return filtered_df

# 각 컬럼에 대해 이상치 제거 적용
for column in data.columns[:-1]:  # 마지막 'Category' 컬럼 제외
    data = remove_outliers(data, column)

# 피처(X)와 타겟(y) 분리
X = data.drop("Category", axis=1)
y = data["Category"]

# 랜덤 포레스트 분류기 생성 및 훈련
rf_clf = RandomForestClassifier(n_estimators=100, random_state=1)
rf_clf.fit(X, y)

# 피처 중요도 추출 및 출력
feature_importances = rf_clf.feature_importances_

# 피처 이름 추출
feature_names = X.columns

# 피처 중요도와 함께 데이터프레임 생성
importances_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importances})

# 중요도에 따라 데이터프레임 정렬
importances_df = importances_df.sort_values(by='Importance', ascending=False)

# 출력
print(importances_df)
```

출력결과
![[Pasted image 20250806205845.png]]
1. 기압(Pressure, 약 41%)*
    - 모델이 가장 크게 참고한 정보입니다.
    - “어떤 기압대일 때 특정 카테고리(음식 종류)를 추천할 가능성이 높은지”를 포레스트가 가장 민감하게 학습했음을 뜻합니다.
        
2. 기온(Temperature, 약 29%)
    - 기온 역시 중요도가 높아, 날씨의 온도 변화가 음식 카테고리 예측에 중요한 역할을 합니다.
        
3. 흐림도(Cloudiness, 약 23%)
    - 하늘 상태(맑음 vs 흐림)도 모델 성능에 꽤 기여하고 있습니다.
        
4. 강수량(Precipitation, 약 7%)
    - 비나 이슬비 정도는 예측에 일부 도움은 주지만, 앞의 세 변수에 비해 영향력은 상대적으로 작습니다.
        
5. 강설량(Snowfall, 0%)
    - 이 데이터셋에서는 눈이 전혀 내리지 않거나, 내린 양이 예측과 무관해 모델이 전혀 사용하지 않았다는 뜻입니다.
        
---
개선・활용 포인트
- Snowfall 제거: 0% 기여 변수는 오히려 노이즈가 될 수 있습니다.
- Pressure·Temperature 중심 재학습: 상위 2~3개 피처만으로 간단한 모델을 만들어 보고, 성능 변화를 살펴보시면 변수 선택 효과를 확인할 수 있습니다.
- 추가적인 피처 엔지니어링: 예를 들어 ‘체감온도’, ‘습도 지수’, ‘계절(봄·여름·가을·겨울)’ 같은 파생 변수를 만들어 넣어 보면 성능 개선에 도움이 될 수 있습니다.
- 이상치 처리 재검토: `remove_outliers` 를 각 컬럼마다 연쇄 적용하면 너무 많은 행이 필터링될 수 있으므로, 한꺼번에 적용하지 말고 변수별로 분리해서 확인해 보세요.

---
데이터의 수치형 변수들 간 상관관계(correlation)를 계산하고,  
그 결과를 히트맵(heatmap)으로 시각화하는 코드
```python
# 데이터 상관관계 보기
import matplotlib.pyplot as plt
import seaborn as sns

# 데이터 로드
data = pd.read_csv('data/weather_data_updated.csv')
X = data.drop("Category", axis=1)
# 데이터 상관관계 계산
correlation_matrix = X.corr()

# 상관관계 히트맵 플롯
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=.5)
plt.title("Feature Correlation Matrix")
plt.show()
```

상관관계 히트맵
![[Pasted image 20250806210328.png]]

| 피처 조합                    | 상관계수  . | 해석                           |
| ------------------------ | ------- | ---------------------------- |
| Temperature–Snowfall     | –0.50   | 기온이 낮을수록 눈이 많이 내림 (강한 음의 상관) |
| Temperature–Pressure     | +0.04   | 거의 상관 없음                     |
| Precipitation–Cloudiness | ~0.00   | 비 올 때와 흐림 정도는 선형 관계가 거의 없음   |
| Cloudiness–Snowfall      | –0.02   | 흐림도와 강설량 역시 선형 관계가 거의 없음     |
이렇게 피처 간 관계를 미리 파악해 두면,
- 강하게 연관된 변수는 중복 제거하거나 파생 변수를 만들어 보고
- 전혀 연관이 없는 변수는 모델에 그대로 넣어도 좋을지
- 또는 새로운 파생 변수를 추가해 볼지를 결정하는 데 큰 도움이 됩니다.

---
데이터의 분포와 변수 간 관계를 한눈에 보기 위해, Plotly의 `scatter_matrix`(산점도 행렬)를 그리는 코드로 결측치 처리와 이상치 제거도 포함
```python
# 데이터 분포도 확인
import plotly.express as px

# 데이터 로드
data = pd.read_csv('data/weather_data_updated.csv')
# 결측치 확인 및 처리
data.isnull().sum()  # 결측치 확인
data.fillna(method='ffill', inplace=True)  # 결측치 처리 (예: 앞의 값으로 채우기)

def remove_outliers(df, column):
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    # 이상치가 아닌 데이터만 필터링
    filtered_df = df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]
    return filtered_df

# 각 컬럼에 대해 이상치 제거 적용
for column in data.columns[:-1]:  # 마지막 'Category' 컬럼 제외
    data = remove_outliers(data, column)
    
fig = px.scatter_matrix(data, 
                        dimensions=["Temperature", "Precipitation", "Cloudiness", "Snowfall", "Pressure"],
                        color="Category", height=800)
fig.show()
```

이 플롯은 카테고리별(음식 종류별)로 색깔을 입힌 “피처 간 2차원 산점도 행렬(scatter matrix)” 입니다.  
각 축이 온도(Temperature), 강수량(Precipitation), 흐림도(Cloudiness), 강설량(Snowfall), 기압(Pressure) 중 하나이고,
- 대각선 위·아래의 각 셀은 두 피처를 x–y 평면에 흩뿌린 산점도입니다.
- 점의 색깔은 그 샘플의 `Category`(분식·디저트·초밥…)를 나타냅니다.
![[Pasted image 20250806210636.png]]
1. 대각선 (Diagonal)
- 자기 자신과의 비교:  
    대각선에 위치한 그래프(예: Temperature vs Temperature, Precipitation vs Precipitation)는 자기 자신과 비교로 의미가 없기때문에 그 칸은 관계대신 음식종류의 분포만 보여줍니다. 그러나 색상이 중식과 파스타등의 색상만 몰려있는것으로 봐서는 데이터의 불균형이 있을수 있다고 판단됩니다. 강설량 같은 경우는 데이터가 없다. 즉 데이터에서 눈온날이 없다고 판단해야 합니다.
    
2. 비대각선 (Off-diagonal)
- 두 변수 간 산점도:  
    예를 들어, "Temperature" (x축) vs "Precipitation" (y축) 위치의 그래프는,  
    모든 데이터에서 온도와 강수량의 관계를 색깔별로(카테고리별로) 산점도로 보여주는 것입니다.
- 가로/세로줄  
    온도와 강설량(Temperature vs Snowfall) 등에서  
    데이터가 x축(온도)는 여러 값인데, y축(강설량)은 거의 0에만 점이 한 줄로 나옴
- 강수량/전운량/기압 등과 강설량, 반대로, Snowfall vs 다른 변수 
    항상 y축(강설량)이 0인 곳에만 점들이 늘어서 있는건 모든 데이터에 눈이 오지 않았다는것을 의미합니다. 그래서 y축 혹은 x축에 0근처에만 데이터가 집중되어 한줄이 된것 입니다.
---
최종 스케일링, 원-핫 인코딩, 결측치 처리, 이상치 제거, 상관관계 분석, 피처 중요도 계산, 시각화
```python
# --------------------------------------------------------------
# 1. 라이브러리 임포트
# --------------------------------------------------------------
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.ensemble import RandomForestClassifier
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px

# --------------------------------------------------------------
# 2. 데이터 불러오기
# --------------------------------------------------------------
data = pd.read_csv('data/weather_data_updated.csv')
print("===== 원본 데이터 미리보기 =====")
print(data.head(), "\n")

# --------------------------------------------------------------
# 3. 결측치 처리 (수치형, 범주형 분리)
# --------------------------------------------------------------
# 결측치 개수 확인
print("===== 컬럼별 결측치 개수 =====")
print(data.isnull().sum(), "\n")

num_cols = data.select_dtypes(include=['float64', 'int64']).columns.tolist()
cat_cols = data.select_dtypes(exclude=['float64', 'int64']).columns.tolist()

print(f"수치형 컬럼: {num_cols}")
print(f"범주형 컬럼: {cat_cols}\n")

# 수치형: 평균으로 채우기, 범주형: 최빈값으로 채우기
num_imputer = SimpleImputer(strategy='mean')
cat_imputer = SimpleImputer(strategy='most_frequent')

data[num_cols] = num_imputer.fit_transform(data[num_cols])
data[cat_cols] = cat_imputer.fit_transform(data[cat_cols])

print("===== 결측치 처리 후 데이터 =====")
print(data.head(), "\n")

# --------------------------------------------------------------
# 4. 이상치 제거 함수 (IQR 방식)
# --------------------------------------------------------------
def remove_outliers(df, column):
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]
    if not outliers.empty:
        print(f"[이상치 제거] 컬럼 '{column}'에서 제거된 행 개수: {len(outliers)}")
    return df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]

# 수치형 컬럼만 적용
for col in num_cols:
    data = remove_outliers(data, col)

print("\n===== 이상치 제거 후 데이터 크기 =====")
print(data.shape, "\n")

# --------------------------------------------------------------
# 5. 피처와 타겟 분리
# --------------------------------------------------------------
# 카테고리 음식종류는 제거하고 5개 피처만 남김
X = data.drop("Category", axis=1) # 피처 추출
y = data["Category"] # 타겟 분리

print("===== 피처(X) 미리보기 =====")
print(X.head(), "\n")
print("===== 타겟(y) 미리보기 =====")
print(y.head(), "\n")

'''
피처 추출/선택/엔지니어링이란?
입력 변수(피처)를 선정하거나 새로 만드는 일
예: 불필요한 피처는 제거,
유용한 정보가 담긴 새 피처 추가,
여러 컬럼을 조합해서 새 변수 만들기 등

이유?
좋은 피처만 골라서 모델 성능을 올리고,
쓸데없는 정보는 제거해서 과적합도 방지하려고!
'''

# --------------------------------------------------------------
# 6. 전처리 컬럼 구분 (X 기준으로) 피처 구분
# --------------------------------------------------------------
num_cols = X.select_dtypes(include=['float64', 'int64']).columns.tolist() 
cat_cols = X.select_dtypes(exclude=['float64', 'int64']).columns.tolist()

# 수치형 컬럼 → StandardScaler() 같은 스케일링 적용   
# 범주형 컬럼 → OneHotEncoder() 같은 인코딩 적용
print(f"전처리 - 수치형 컬럼: {num_cols}")
print(f"전처리 - 범주형 컬럼: {cat_cols}\n")

# 전처리 파이프라인
# ColumnTransformer : 
# 데이터의 여러 컬럼(수치형, 범주형 등)에 서로 다른 전처리 방법을
# 한 번에 적용할 수 있게 도와주는 도구
# 범위가 넓은 수치는 스케일링으로 좁히고, 범주형은 원-핫 인코딩으로 변환
preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), num_cols), # 수치형 → 스케일링
        ('cat', OneHotEncoder(handle_unknown='ignore'), cat_cols) 
        # 범주형 → 원핫인코딩
    ]
)

# --------------------------------------------------------------
# 7. 전처리 적용
# --------------------------------------------------------------
X_processed = preprocessor.fit_transform(X)
print("===== 전처리(스케일링+인코딩) 적용 후 X =====")
print(f"전처리 후 shape: {X_processed.shape}\n")

# --------------------------------------------------------------
# 8. EDA 1 - 피처 중요도 분석 (수치형만)
# --------------------------------------------------------------
rf_clf = RandomForestClassifier(n_estimators=100, random_state=1)
rf_clf.fit(X[num_cols], y)  # 원본 수치형만 사용해서 중요도 분석

feature_importances = rf_clf.feature_importances_
importances_df = pd.DataFrame({'Feature': num_cols, 'Importance': feature_importances})
importances_df = importances_df.sort_values(by='Importance', ascending=False)

print("\n===== 피처 중요도 순위 =====")
print(importances_df, "\n")

plt.figure(figsize=(8, 5))
plt.barh(importances_df['Feature'], importances_df['Importance'])
plt.xlabel("Importance")
plt.title("Feature Importance in Random Forest")
plt.gca().invert_yaxis()
plt.show()

# --------------------------------------------------------------
# 9. EDA 2 - 상관관계 히트맵
# --------------------------------------------------------------
correlation_matrix = X[num_cols].corr()
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=.5)
plt.title("Feature Correlation Matrix (Numerical Data)")
plt.show()

# --------------------------------------------------------------
# 10. EDA 3 - 데이터 분포 (Plotly)
# --------------------------------------------------------------
fig = px.scatter_matrix(
    data,
    dimensions=num_cols,
    color="Category",
    height=800
)
fig.show()

# --------------------------------------------------------------
# 11. 모델 학습 및 평가
# --------------------------------------------------------------
X_train, X_test, y_train, y_test = train_test_split(
	X_processed, y, test_size=0.2, random_state=1
)

rf = RandomForestClassifier(n_estimators=100, random_state=2)
rf.fit(X_train, y_train)

# --------------------------------------------------------------
# 교차 검증 (Cross-Validation)
# --------------------------------------------------------------
# 기본값 scoring='accuracy' → 정확도 기반 검증
k_fold_scores = cross_val_score(rf, X_processed, y, cv=5)

print("\n===== 모델 성능 검증 결과 =====")
print("▶ 사용한 검증 방식: 5-Fold Cross Validation")
print("▶ 평가 지표: 정확도(Accuracy)")
print(f"▶ 각 Fold별 정확도 점수: {k_fold_scores}")
print(f"▶ 평균 정확도(5개 Fold 평균): {k_fold_scores.mean():.4f}")
print("※ 평균 정확도는 모델이 전체 데이터에 대해 얼마나 잘 맞추는지를 나타내는 지표입니다.\n")

# --------------------------------------------------------------
# 12. 새로운 날씨 데이터 예측 예시
# --------------------------------------------------------------
new_weather_data = pd.DataFrame([[-2, 0, 100, 0, 993, '분식']], 
    columns=['Temperature', 'Precipitation', 'Cloudiness', 'Snowfall', 'Pressure', 'Category'])

print("===== 새 입력 데이터(원본) =====")
print(new_weather_data, "\n")

scaled_new_data = preprocessor.transform(new_weather_data)
print("===== 새 입력 데이터(전처리 후) =====")
print(scaled_new_data, "\n")

probs = rf.predict_proba(scaled_new_data)
print("===== 예측 확률 =====")
print(probs, "\n")

best_two = np.argsort(probs, axis=1)[:,-2:]
top_categories = [rf.classes_[i] for i in best_two[0][::-1]]

print("New Data Top 2 Predicted Categories:", top_categories)

# --------------------------------------------------------------
# 13. 학습된 전처리 파이프라인과 학습된 랜덤포레스트 모델 저장
# --------------------------------------------------------------
from joblib import dump

# 학습된 전처리 파이프라인
dump(preprocessor, "models/preprocessor.joblib")

# 학습된 RandomForest 모델
dump(rf, "models/random_forest_model.joblib")
# 2개를 저장하면, 다음부터는 데이터를 새로 읽고 전처리 및 모델 훈련 없이 .transform()과 .predict()만으로 예측 가능해집니다.
```

성능 점수
```
===== 모델 성능 검증 결과 =====
▶ 사용한 검증 방식: 5-Fold Cross Validation
▶ 평가 지표: 정확도(Accuracy)
▶ 각 Fold별 정확도 점수: 
[0.29568106 0.31  0.30666667  0.34  0.34666667]
▶ 평균 정확도(5개 Fold 평균): 0.3198
※ 평균 정확도는 모델이 전체 데이터에 대해 얼마나 잘 맞추는지를 나타내는 지표입니다.
```

---
성능평가 결과가 낮게 나왔으므로 이후에 시도해봐야 할 작업들을 정리하였습니다.

성능 향상을 위한 포인트
`1.` 이진 플래그 (Binary Flags)
- `is_rain = Precipitation > 0`
- `is_snow = Snowfall > 0`
- `high_cloud = Cloudiness > 50`

```python
data['is_rain'] = (data['Precipitation'] > 0).astype(int) data['is_snow'] = (data['Snowfall']    > 0).astype(int) data['high_cloud'] = (data['Cloudiness']  > 50).astype(int)
```
> 비가 오면 분식보다 찜·국밥을 더 찾을 것 같은 뚜렷한 패턴이 이진 플래그로 더 잘 잡힙니다.
---
`2.` 구간화(Binning)된 범주형 피처
- 온도, 기압을 쾌적/덥/춥, 저기압/보통/고기압 식으로 구간 나눠 범주형으로 변환

```python
# 예: 온도 구간
bins = [-np.inf, 0, 10, 20, 30, np.inf]
labels = ['very_cold','cold','mild','warm','hot']
data['temp_bin'] = pd.cut(data['Temperature'], bins=bins, labels=labels)

# 예: 기압 구간
data['pressure_bin'] = pd.qcut(data['Pressure'], q=3, labels=['low','med','high'])
```
> 20~25℃ 사이엔 카페가 더 많이 선택된다 같은 범주 패턴을 모델이 쉽게 학습합니다.
---
`3.` 상호작용 피처(Interaction Terms)
- 서로 상관이 낮은 피처끼리 곱하거나 나누어 새로운 정보를 만듭니다

```python
data['temp_cloud_inter'] = data['Temperature'] * data['Cloudiness']
data['rain_pressure_ratio'] = data['Precipitation'] / (data['Pressure'] + 1e-3)
```
> 높은 구름도 + 낮은 기압 조합이 특정 음식을 선호하는 날씨 그룹일 수 있습니다.
---
`4.` 파생 지수 (Derived Indices)
- 실제 체감온도(Heat Index)나 불쾌지수(Discomfort Index) 계산

```python
# 간단 체감온도 예시 (섭씨)
T = data['Temperature']
H = data.get('Humidity', 50)  # 습도 컬럼이 있다면 사용
data['heat_index'] = 0.5*(T + 61.0 + ((T-68.0)*1.2) + (H*0.094))
```
> 습도가 높으면 더 불쾌해서 찜·찌개 등 따끈한 걸 더 찾는다 같은 미묘한 온습도 상관관계를 반영할 수 있습니다.
---
 `5.` 군집 레이블 (Clustering)
- KMeans 등으로 날씨 패턴 클러스터링 후, cluster label을 추가

```python
from sklearn.cluster import KMeans
km = KMeans(n_clusters=4, random_state=1)
data['weather_cluster'] = km.fit_predict(data[num_cols])
```
> 모델이 수작업으로 파생하기 어려운 복합 날씨 조건(“맑고 덥고 저기압”) 그룹을 스스로 발견하게 해 줍니다.
---
`6.` 계절·요일 정보 (시계열 파생)
- 날짜 컬럼이 있다면 `month`, `weekday`, `is_weekend` 추가
    
```python
data['month'] = data['date'].dt.month
data['is_weekend'] = data['date'].dt.weekday.isin([5,6]).astype(int)
```
> 여름 주말엔 파스타·초밥이, 겨울 평일엔 국밥·찜이 더 많이 선택되는 경향을 모델이 학습할 수 있습니다.

`7.` Cold Start 상황에서 Rule-Based 방식 병행 추천 방법
- 초반에는 사용자 행동 데이터가 부족해서 머신러닝 성능이 낮습니다.
- 이를 보완하기 위해 Rule-Based 추천을 추가해서 안정적인 추천을 제공할 수 있습니다.
```
project/
├── models/
│   ├── random_forest_model.joblib
│   └── preprocessor.joblib
│
├── rules/
│   └── rule_config.json
│
├── recommender/
│   ├── hybrid.py              ← 하이브리드 추천 함수 모듈
│   └── rule_utils.py          ← 룰 기반 로직만 따로 분리
│
├── main.py                    ← 추천 테스트 코드 or API에 연결
└── data/
    └── weather_data_updated.csv
```

```json
{
    "cold_weather": "국밥",
    "hot_weather": "디저트",
    "rainy": "라멘",
    "cloudy": "카페",
    "default": "분식"
}
```

recommender/rule_utils.py
```python
import json

def load_rules(path="rules/rule_config.json"):
    with open(path, "r", encoding="utf-8") as f:
        return json.load(f)

def rules_based_recommendation(row, rules):
    category_rules = []

    if row["Snowfall"] > 5:
        category_rules.append(rules.get("cold_weather", "국밥"))
    if row["Temperature"] > 28:
        category_rules.append(rules.get("hot_weather", "디저트"))
    if row["Cloudiness"] > 80:
        category_rules.append(rules.get("cloudy", "카페"))
    if row["Precipitation"] > 20:
        category_rules.append(rules.get("rainy", "라멘"))

    return category_rules if category_rules else [rules.get("default", "분식")]
```

recommender/hybrid.py
```python
import numpy as np
from recommender.rule_utils import rules_based_recommendation

def hybrid_recommender(new_data_df, model, preprocessor, rules):
    """
    - new_data_df: 입력 데이터 (DataFrame)
    - model: 학습된 ML 모델
    - preprocessor: 전처리기
    - rules: 룰 기반 추천 dict
    """
    try:
        processed = preprocessor.transform(new_data_df)
        probs = model.predict_proba(processed)
        top2_idx = np.argsort(probs, axis=1)[:,-2:]
        top2 = [model.classes_[i] for i in top2_idx[0][::-1]]
    except Exception as e:
        print("[ML 예측 실패] Rule-Based만 사용합니다:", e)
        top2 = []

    rule_based_result = rules_based_recommendation(new_data_df.iloc[0], rules)

    hybrid = list(dict.fromkeys(top2 + rule_based_result)) 
    # 중복 제거
    
    return hybrid[:2]
```

`main.py` — 추천 실행 예제
```python
import pandas as pd
from joblib import load
from recommender.hybrid import hybrid_recommender
from recommender.rule_utils import load_rules

# 모델 로딩
model = load("models/random_forest_model.joblib")
preprocessor = load("models/preprocessor.joblib")
rules = load_rules("rules/rule_config.json")

# 테스트 데이터
new_weather_data = pd.DataFrame([[-2, 0, 100, 10, 993, '기타']],
    columns=['Temperature', 'Precipitation', 'Cloudiness', 'Snowfall', 'Pressure', 'Category'])

# 추천 실행
result = hybrid_recommender(new_weather_data, model, preprocessor, rules)
print("추천 결과:", result)
```